From ebd53aa9057c73556a7cf83a9114a46c2770d131 Mon Sep 17 00:00:00 2001
From: Haojun Xia <xhjustc@mail.ustc.edu.cn>
Date: Mon, 12 Dec 2022 23:44:00 +1100
Subject: [PATCH] Extending faster-transformer to supoort pruned GPT model
 inference by replacing cuBLAS with our FlashLLM kernels.

---
 CMakeLists.txt                                |  19 +-
 examples/cpp/multi_gpu_gpt/gpt_config.ini     |  66 ++-
 .../multi_gpu_gpt/multi_gpu_gpt_example.cc    | 104 ++--
 examples/cpp/multi_gpu_gpt/start_ids.csv      |   4 +-
 .../cpp/multi_gpu_gpt/start_ids_backup.csv    |   8 +
 src/fastertransformer/layers/DenseWeight.h    |  14 +-
 src/fastertransformer/layers/FfnLayer.cc      | 230 +++++----
 .../DecoderSelfAttentionLayer.cc              | 468 ++++++++++--------
 .../GptContextAttentionLayer.cc               | 176 ++++---
 .../ParallelGptDecoderLayerWeight.cc          | 429 +++++++++++-----
 .../ParallelGptDecoderLayerWeight.h           |  65 ++-
 .../models/multi_gpu_gpt/ParallelGptWeight.cc | 225 +++++++--
 .../models/multi_gpu_gpt/ParallelGptWeight.h  |  66 ++-
 src/fastertransformer/utils/CMakeLists.txt    |   4 +
 .../utils/cublasMMWrapper.cc                  | 444 +++++++++--------
 src/fastertransformer/utils/cublasMMWrapper.h | 250 +++++-----
 src/fastertransformer/utils/memory_utils.cu   | 123 ++++-
 src/fastertransformer/utils/memory_utils.h    |  13 +-
 18 files changed, 1717 insertions(+), 991 deletions(-)
 create mode 100644 examples/cpp/multi_gpu_gpt/start_ids_backup.csv

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 76a5e06..3da02f0 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -31,6 +31,8 @@ if(NOT USE_TRITONSERVER_DATATYPE)
   option(USE_TRITONSERVER_DATATYPE "Build triton backend for triton server" OFF)
 endif()
 
+option(FLASH_LLM "Build project with FlashLLM feature support" OFF)
+
 option(SPARSITY_SUPPORT "Build project with Ampere sparsity feature support" OFF)
 
 option(BUILD_FAST_MATH "Build in fast math mode" ON)
@@ -161,7 +163,7 @@ set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -O3")
 set(CMAKE_CUDA_FLAGS_RELEASE "${CMAKE_CUDA_FLAGS_RELEASE} -Xcompiler -O3")
 if(BUILD_FAST_MATH)
 set(CMAKE_CUDA_FLAGS_RELEASE "${CMAKE_CUDA_FLAGS_RELEASE} --use_fast_math")
-message("CMAKE_CUDA_FLAGS_RELEASE: ${CMAKE_CUDA_FLAGS_RELEASE}")
+message("-- CMAKE_CUDA_FLAGS_RELEASE: ${CMAKE_CUDA_FLAGS_RELEASE}")
 endif()
 
 set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
@@ -179,11 +181,26 @@ set(COMMON_LIB_DIRS
 )
 
 if (SPARSITY_SUPPORT)
+  message("SPARSITY_SUPPORT is ON!")
   list(APPEND COMMON_HEADER_DIRS ${CUSPARSELT_PATH}/include)
   list(APPEND COMMON_LIB_DIRS ${CUSPARSELT_PATH}/lib64)
   add_definitions(-DSPARSITY_ENABLED=1)
 endif()
 
+#######################################################
+#set(SPARSITY_HAOJUN "OFF")
+if (FLASH_LLM)
+  message("-- FLASH_LLM is ${FLASH_LLM}!")
+  message("-- FLASH_LLM_HOME is $ENV{FlashLLM_HOME}!")
+  add_definitions(-DSPARSITY_HAOJUN=1)
+  include_directories($ENV{FlashLLM_HOME}/build)
+  link_directories($ENV{FlashLLM_HOME}/build)
+endif()
+# Decided what to print during model execution
+#add_definitions(-DPRINT_DEBUG_INFO=1)
+#add_definitions(-DPRINT_MISSING_INPUT_FILES=1)
+#######################################################
+
 if(BUILD_TF)
   list(APPEND COMMON_HEADER_DIRS ${TF_PATH}/include)
   list(APPEND COMMON_LIB_DIRS ${TF_PATH})
diff --git a/examples/cpp/multi_gpu_gpt/gpt_config.ini b/examples/cpp/multi_gpu_gpt/gpt_config.ini
index 72cfc07..76961a7 100644
--- a/examples/cpp/multi_gpu_gpt/gpt_config.ini
+++ b/examples/cpp/multi_gpu_gpt/gpt_config.ini
@@ -1,38 +1,34 @@
 [ft_instance_hyperparameter]
-max_batch_size=8 ; Use for allocate the buffer
+max_batch_size=256 ; Use for allocate the buffer
 max_seq_len=1024 ; The sequence length of position embedding table, should move to model hyper-parameter
 beam_width=1 ; beam width for beam search
 top_k=1 ; k value for top k sampling
 top_p=0 ; p value for top p sampling
 temperature=1.0 ; Use for sampling
 repetition_penalty=1.0 ; Use for sampling
-tensor_para_size=1
 pipeline_para_size=1
 data_type=fp16
 sparse=0
 int8_mode=0
 enable_custom_all_reduce=0
-; model_name=gpt_124M
-model_name=megatron_345M
-; model_name=megatron_1.3B_adapter
-; model_name=megatron_6.7B
-; model_name=megatron_20B
-; model_name=gpt_175B
-; model_name=opt_125M
-; model_name=opt_350M
-; model_name=self_defined
-; model_dir=./models/megatron-models/c-model/6.7b/
-model_dir=../models/megatron-models/c-model/345m/1-gpu/
+;
 len_penalty=0.0
 beam_search_diversity_rate=0.0
-shared_contexts_ratio=1.0
+shared_contexts_ratio=0.0   ; disable the optimization of shared_contexts, otherwise the context input pre-processing will be very complicated here
+return_log_probs=false   ; return the output log probs and cumulative log probs.
+context_log_probs=false  ; include input contexts in the cumulative log probability computation.
+remove_padding=false     ; must set to false, otherwise strange shape will be caused for SpMM ; [Haojun]
 
+;!!!!!!!!!!!!!!!!!!!!!! Choosing your models here !!!!!!!!!!!!!!!!!!!!!!
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+;Model Choosing
+model_name=opt_30B
+tensor_para_size=1
+model_dir = /data2/Haojun/opt-30b/c-model/1-gpu
+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
 [request]
-request_batch_size=8    ; determine by the request
-request_output_len=32   ; determine by the request
-return_log_probs=false  ; return the output log probs and cumulative log probs.
-context_log_probs=false ; include input contexts in the cumulative log probability computation.
-remove_padding=true
+request_batch_size=64    ; determine by the request
+request_output_len=512   ; determine by the request
 
 [gpt_124M]
 head_num=12
@@ -125,7 +121,37 @@ decoder_layers=24
 start_id=2
 end_id=2
 inter_size=4096
-model_variant=opt-post
+model_variant=opt-post ;  special in whole OPT family
+
+[opt_30B]
+head_num=56
+size_per_head=128
+vocab_size=50272
+decoder_layers=48
+start_id=2
+end_id=2
+inter_size=28672
+model_variant=opt-pre ;define variant structure
+
+[opt_66B]
+head_num=72
+size_per_head=128
+vocab_size=50272
+decoder_layers=64
+start_id=2
+end_id=2
+inter_size=36864
+model_variant=opt-pre ;define variant structure
+
+[opt_175B]
+head_num=96
+size_per_head=128
+vocab_size=50272
+decoder_layers=96
+start_id=2
+end_id=2
+inter_size=49152
+model_variant=opt-pre ;define variant structure
 
 [self_defined]
 head_num=16
diff --git a/examples/cpp/multi_gpu_gpt/multi_gpu_gpt_example.cc b/examples/cpp/multi_gpu_gpt/multi_gpu_gpt_example.cc
index acd75e7..4b884a0 100644
--- a/examples/cpp/multi_gpu_gpt/multi_gpu_gpt_example.cc
+++ b/examples/cpp/multi_gpu_gpt/multi_gpu_gpt_example.cc
@@ -87,53 +87,53 @@ int main(int argc, char* argv[])
 template<typename T>
 void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
 {
-    const std::string model_name         = reader.Get("ft_instance_hyperparameter", "model_name");
-    const size_t      max_batch_size     = (size_t)reader.GetInteger("ft_instance_hyperparameter", "max_batch_size");
-    const size_t      max_seq_len        = (size_t)reader.GetInteger("ft_instance_hyperparameter", "max_seq_len");
-    const size_t      beam_width         = (size_t)reader.GetInteger("ft_instance_hyperparameter", "beam_width");
-    const uint        top_k              = (uint)reader.GetInteger("ft_instance_hyperparameter", "top_k");
-    const float       top_p              = reader.GetFloat("ft_instance_hyperparameter", "top_p");
-    const float       temperature        = reader.GetFloat("ft_instance_hyperparameter", "temperature");
-    const float       repetition_penalty = reader.GetFloat("ft_instance_hyperparameter", "repetition_penalty");
-    const std::string model_dir          = std::string(reader.Get("ft_instance_hyperparameter", "model_dir"));
-    const bool        sparse             = static_cast<bool>(reader.GetInteger("ft_instance_hyperparameter", "sparse"));
-    const int         int8_mode          = reader.GetInteger("ft_instance_hyperparameter", "int8_mode");
-    const float       len_penalty        = reader.GetFloat("ft_instance_hyperparameter", "len_penalty");
-    const float       beam_search_diversity_rate =
+    const std::string model_name = reader.Get("ft_instance_hyperparameter", "model_name");
+    const size_t max_batch_size = (size_t)reader.GetInteger("ft_instance_hyperparameter", "max_batch_size");
+    const size_t max_seq_len = (size_t)reader.GetInteger("ft_instance_hyperparameter", "max_seq_len");
+    const size_t beam_width = (size_t)reader.GetInteger("ft_instance_hyperparameter", "beam_width");
+    const uint top_k = (uint)reader.GetInteger("ft_instance_hyperparameter", "top_k");
+    const float top_p = reader.GetFloat("ft_instance_hyperparameter", "top_p");
+    const float temperature = reader.GetFloat("ft_instance_hyperparameter", "temperature");
+    const float repetition_penalty = reader.GetFloat("ft_instance_hyperparameter", "repetition_penalty");
+    const std::string model_dir = std::string(reader.Get("ft_instance_hyperparameter", "model_dir"));
+    const bool sparse = static_cast<bool>(reader.GetInteger("ft_instance_hyperparameter", "sparse"));
+    const int int8_mode = reader.GetInteger("ft_instance_hyperparameter", "int8_mode");
+    const float len_penalty = reader.GetFloat("ft_instance_hyperparameter", "len_penalty");
+    const float beam_search_diversity_rate =
         reader.GetFloat("ft_instance_hyperparameter", "beam_search_diversity_rate");
     const float shared_contexts_ratio = reader.GetFloat("ft_instance_hyperparameter", "shared_contexts_ratio", true);
 
-    const int tensor_para_size   = reader.GetInteger("ft_instance_hyperparameter", "tensor_para_size");
+    const int tensor_para_size = reader.GetInteger("ft_instance_hyperparameter", "tensor_para_size");
     const int pipeline_para_size = reader.GetInteger("ft_instance_hyperparameter", "pipeline_para_size");
 
-    const size_t      head_num       = (size_t)reader.GetInteger(model_name, "head_num");
-    const size_t      size_per_head  = (size_t)reader.GetInteger(model_name, "size_per_head");
-    const size_t      vocab_size     = (size_t)reader.GetInteger(model_name, "vocab_size");
-    const size_t      decoder_layers = (size_t)reader.GetInteger(model_name, "decoder_layers");
-    const size_t      hidden_units   = head_num * size_per_head;
-    const size_t      inter_size     = 4 * hidden_units;
-    const std::string model_variant  = std::string(reader.Get(model_name, "model_variant", "gpt"));
+    const size_t head_num = (size_t)reader.GetInteger(model_name, "head_num");
+    const size_t size_per_head = (size_t)reader.GetInteger(model_name, "size_per_head");
+    const size_t vocab_size = (size_t)reader.GetInteger(model_name, "vocab_size");
+    const size_t decoder_layers = (size_t)reader.GetInteger(model_name, "decoder_layers");
+    const size_t hidden_units = head_num * size_per_head;
+    const size_t inter_size = 4 * hidden_units;
+    const std::string model_variant = std::string(reader.Get(model_name, "model_variant", "gpt"));
 
     const size_t request_batch_size = reader.GetInteger("request", "request_batch_size");
     // The length of tokens we hope this model to generate
-    const int  request_output_len  = reader.GetInteger("request", "request_output_len");
+    const int request_output_len = reader.GetInteger("request", "request_output_len");
     const bool is_return_log_probs = reader.GetBoolean("request", "return_log_probs", false);
     // Whether to include input contexts in computing the cumulative log probabilities.
     const bool is_return_context_cum_log_probs = reader.GetBoolean("request", "context_log_probs", false);
     if (is_return_log_probs && !is_return_context_cum_log_probs) {
         FT_LOG_WARNING("context_log_probs will be ignored since return_log_probs is disabled.");
     }
-    const bool     remove_padding = reader.GetBoolean("request", "remove_padding", false);
-    const uint32_t memory_len     = reader.GetInteger("request", "memory_len", 0);
+    const bool remove_padding = reader.GetBoolean("request", "remove_padding", false);
+    const uint32_t memory_len = reader.GetInteger("request", "memory_len", 0);
 
     const int start_id = 50256;
-    const int end_id   = 50256;
+    const int end_id = 50256;
 
     FT_CHECK(head_num % tensor_para_size == 0);
     FT_CHECK(decoder_layers % pipeline_para_size == 0);
 
     // Prepare the parallelism parameters
-    int rank       = mpi::getCommWorldRank();
+    int rank = mpi::getCommWorldRank();
     int world_size = mpi::getCommWorldSize();
     if (rank == 0) {
         printf("Total ranks: %d.\n", world_size);
@@ -171,18 +171,19 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     ftNcclInitialize(tensor_para, pipeline_para, tensor_para_size, pipeline_para_size);
 
     // Read ids of request from file.
-    int              max_input_len = -1;
+    int max_input_len = -1;
     std::vector<int> v_start_lengths;
     std::vector<int> v_start_ids;
     read_start_ids(request_batch_size, &v_start_lengths, &v_start_ids, max_input_len, end_id, 1, in_csv);
 
     int* d_input_ids;
     int* d_input_lengths;
+    printf("multi_gpu_gpt_example.cc: max_input_len = %d.\n", max_input_len);
     if (max_input_len == 0) {
         // unconditional case, no input ids, so do nothing.
-        d_input_ids     = nullptr;
+        d_input_ids = nullptr;
         d_input_lengths = nullptr;
-        max_input_len   = 0;
+        max_input_len = 0;
     }
     else {
         // conditional case.
@@ -198,8 +199,8 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
         exit(-1);
     }
 
-    cudaStream_t     stream;
-    cublasHandle_t   cublas_handle;
+    cudaStream_t stream;
+    cublasHandle_t cublas_handle;
     cublasLtHandle_t cublaslt_handle;
     cudaStreamCreate(&stream);
     cublasCreate(&cublas_handle);
@@ -210,7 +211,7 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     CHECK_CUSPARSE(cusparseLtInit(&cusparselt_handle));
     cublasAlgoMap* cublas_algo_map = new cublasAlgoMap(GEMM_CONFIG, SPGEMM_CONFIG);
 #else
-    cublasAlgoMap*  cublas_algo_map = new cublasAlgoMap(GEMM_CONFIG);
+    cublasAlgoMap* cublas_algo_map = new cublasAlgoMap(GEMM_CONFIG);
 #endif
 
     Allocator<AllocatorType::CUDA> allocator(getDevice());
@@ -237,7 +238,7 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     }
 
     // Prompt Learning Configurations
-    int                prompt_learning_start_id = reader.GetInteger(model_name, "prompt_learning_start_id", end_id + 1);
+    int prompt_learning_start_id = reader.GetInteger(model_name, "prompt_learning_start_id", end_id + 1);
     PromptLearningType prompt_learning_type =
         static_cast<PromptLearningType>(reader.GetInteger(model_name, "prompt_learning_type", 0));
 
@@ -253,8 +254,8 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     const int num_tasks = reader.GetInteger(model_name, "num_tasks", 0);
     for (int task_name_id = 0; task_name_id < num_tasks; task_name_id++) {
         std::string config_task_name = model_name + "_task_" + std::to_string(task_name_id);
-        std::string task_name        = reader.Get(config_task_name, "task_name");
-        const int   prompt_length    = reader.GetInteger(config_task_name, "prompt_length", 0);
+        std::string task_name = reader.Get(config_task_name, "task_name");
+        const int prompt_length = reader.GetInteger(config_task_name, "prompt_length", 0);
         p_prompt_tuning_table_pair_.insert({task_name, {task_name_id, prompt_length}});
     }
 
@@ -265,20 +266,20 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     // NOTE: gpt variants parameters --> meta opt as an example here
     gptVariantParams gpt_variant_params = {};  // default is gpt
     if (model_variant == "opt-pre") {
-        gpt_variant_params.layernorm_eps              = 1e-5f;
-        gpt_variant_params.layernorm_type             = LayerNormType::pre_layernorm;
-        gpt_variant_params.activation_type            = ActivationType::Relu;
+        gpt_variant_params.layernorm_eps = 1e-5f;
+        gpt_variant_params.layernorm_type = LayerNormType::pre_layernorm;
+        gpt_variant_params.activation_type = ActivationType::Relu;
         gpt_variant_params.has_post_decoder_layernorm = true;
     }
     else if (model_variant == "opt-post") {
-        gpt_variant_params.layernorm_eps              = 1e-5f;
-        gpt_variant_params.layernorm_type             = LayerNormType::post_layernorm;
-        gpt_variant_params.activation_type            = ActivationType::Relu;
+        gpt_variant_params.layernorm_eps = 1e-5f;
+        gpt_variant_params.layernorm_type = LayerNormType::post_layernorm;
+        gpt_variant_params.activation_type = ActivationType::Relu;
         gpt_variant_params.has_post_decoder_layernorm = false;
     }
-    gpt_variant_params.has_adapters       = reader.GetBoolean(model_name, "has_adapters", false);
+    gpt_variant_params.has_adapters = reader.GetBoolean(model_name, "has_adapters", false);
     gpt_variant_params.adapter_inter_size = reader.GetInteger(model_name, "adapter_inter_size", inter_size);
-    gpt_variant_params.layernorm_eps      = reader.GetInteger(model_name, "layernorm_eps", 1e-6f);
+    gpt_variant_params.layernorm_eps = reader.GetInteger(model_name, "layernorm_eps", 1e-6f);
 
     ParallelGptWeight<T> gpt_weights(hidden_units,
                                      inter_size,
@@ -289,6 +290,9 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
                                      tensor_para.rank_,
                                      pipeline_para.world_size_,
                                      pipeline_para.rank_,
+#ifdef SPARSITY_HAOJUN
+                                     model_dir,
+#endif
                                      int8_mode,
                                      prompt_learning_type,
                                      p_prompt_tuning_table_pair_,
@@ -395,7 +399,7 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
          Tensor{MEMORY_GPU, TYPE_INT32, std::vector<size_t>{request_batch_size, beam_width}, d_sequence_lengths}}};
 
     float* output_log_probs = nullptr;
-    float* d_cum_log_probs  = nullptr;
+    float* d_cum_log_probs = nullptr;
     if (is_return_log_probs) {
         deviceMalloc(&output_log_probs, request_batch_size * beam_width * request_output_len);
         output_tensors.insert({"output_log_probs",
@@ -432,14 +436,14 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     nvtx::resetScope();
 
     if (rank == 0) {
-        std::string fName   = "out";
-        auto        outFile = std::ofstream(fName, std::ios::out);
+        std::string fName = "out";
+        auto outFile = std::ofstream(fName, std::ios::out);
         if (!outFile.is_open()) {
             printf("[WARNING] Cannot write results into output file %s \n", fName.c_str());
         }
         else {
             size_t outCount = total_output_len * request_batch_size * beam_width;
-            int*   hBuf     = new int[outCount];
+            int* hBuf = new int[outCount];
             cudaD2Hcpy(hBuf, d_output_ids, outCount);
 
             {
@@ -468,8 +472,8 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
         outFile.close();
 
         if (d_cum_log_probs != nullptr) {
-            std::string   logprob_fname = "logprob.out";
-            std::ofstream logprob_file  = std::ofstream("logprob.out", std::ios::out);
+            std::string logprob_fname = "logprob.out";
+            std::ofstream logprob_file = std::ofstream("logprob.out", std::ios::out);
             if (!logprob_file.is_open()) {
                 printf("[WARNING] Cannot write results into output file %s \n", logprob_fname.c_str());
             }
@@ -496,7 +500,7 @@ void multi_gpu_gpt_example(const INIReader reader, std::string in_csv)
     cudaDeviceSynchronize();
     gettimeofday(&start, NULL);
 
-    ite = 10;
+    ite = 1;
 
     nvtx::setScope("total_time");
     PUSH_RANGE("total time")
diff --git a/examples/cpp/multi_gpu_gpt/start_ids.csv b/examples/cpp/multi_gpu_gpt/start_ids.csv
index 38c1beb..82f8aea 100644
--- a/examples/cpp/multi_gpu_gpt/start_ids.csv
+++ b/examples/cpp/multi_gpu_gpt/start_ids.csv
@@ -1,8 +1,8 @@
-818, 262, 938, 3155, 286, 1528, 11, 257
+818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257, 818, 262, 938, 3155, 286, 1528, 11, 257
 198, 464, 968, 8221, 2732, 286, 15198, 318
 464, 968, 1971, 12056, 423, 257, 649, 1182
 464, 968, 1971, 3782, 468, 3199, 663, 5079
 818, 257, 1445, 326, 481, 1884, 787, 340
 464, 968, 1971, 12056, 6, 5859, 41683, 423
 198, 198, 464, 5398, 4332, 628, 628, 198
-464, 717, 640, 314, 2497, 262, 3807, 11
+464, 717, 640, 314, 2497, 262, 3807, 11
\ No newline at end of file
diff --git a/examples/cpp/multi_gpu_gpt/start_ids_backup.csv b/examples/cpp/multi_gpu_gpt/start_ids_backup.csv
new file mode 100644
index 0000000..38c1beb
--- /dev/null
+++ b/examples/cpp/multi_gpu_gpt/start_ids_backup.csv
@@ -0,0 +1,8 @@
+818, 262, 938, 3155, 286, 1528, 11, 257
+198, 464, 968, 8221, 2732, 286, 15198, 318
+464, 968, 1971, 12056, 423, 257, 649, 1182
+464, 968, 1971, 3782, 468, 3199, 663, 5079
+818, 257, 1445, 326, 481, 1884, 787, 340
+464, 968, 1971, 12056, 6, 5859, 41683, 423
+198, 198, 464, 5398, 4332, 628, 628, 198
+464, 717, 640, 314, 2497, 262, 3807, 11
diff --git a/src/fastertransformer/layers/DenseWeight.h b/src/fastertransformer/layers/DenseWeight.h
index d787baa..b9d7a1d 100644
--- a/src/fastertransformer/layers/DenseWeight.h
+++ b/src/fastertransformer/layers/DenseWeight.h
@@ -20,12 +20,20 @@ namespace fastertransformer {
 
 template<typename T>
 struct DenseWeight {
-    const T* kernel    = nullptr;
-    const T* bias      = nullptr;
+    const T* kernel = nullptr;
+    const T* bias = nullptr;
     const T* sp_kernel = nullptr;
     // for int8 kernel
     const int8_t* int8_kernel = nullptr;
-    const float*  scale       = nullptr;
+    const float* scale = nullptr;
+    //
+#ifdef SPARSITY_HAOJUN
+    int SplitK = -1;                          // SplitK
+    int NNZ = -1;                             // Number of Non-Zeros in this spase matrix
+    int NumOffsets = -1;                      // Number of TileOffsets after padding (+2)
+    const unsigned int* NZWeights = nullptr;  // Non-Zero Weights, NZWeights[NNZ]
+    const int* TileOffsets = nullptr;         // TileOffsets[ (M/TILE_M)*(K/TILE_K)+2 ]
+#endif
 };
 
 }  // namespace fastertransformer
\ No newline at end of file
diff --git a/src/fastertransformer/layers/FfnLayer.cc b/src/fastertransformer/layers/FfnLayer.cc
index 509b183..21be9e3 100644
--- a/src/fastertransformer/layers/FfnLayer.cc
+++ b/src/fastertransformer/layers/FfnLayer.cc
@@ -19,9 +19,9 @@
 namespace fastertransformer {
 
 template<typename T>
-void FfnLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_tensors,
+void FfnLayer<T>::forward(std::vector<fastertransformer::Tensor>* output_tensors,
                           const std::vector<fastertransformer::Tensor>* input_tensors,
-                          const FfnWeight<T>*                           ffn_weights)
+                          const FfnWeight<T>* ffn_weights)
 {
     // input tensors:
     //      ffn_input [token_num, hidden_dimension],
@@ -35,9 +35,9 @@ void FfnLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_t
     // FT_CHECK(isValidTokenNum(input_tensors->at(0).shape[0]));
     allocateBuffer(input_tensors->at(0).shape[0]);
 
-    const int m             = input_tensors->at(0).shape[0];
-    T*        output_tensor = (T*)output_tensors->at(0).data;
-    const T*  input_tensor  = (const T*)input_tensors->at(0).data;
+    const int m = input_tensors->at(0).shape[0];
+    T* output_tensor = (T*)output_tensors->at(0).data;
+    const T* input_tensor = (const T*)input_tensors->at(0).data;
 
     // TODO: INT8 and Sparsity are currently not implemented (geglu or reglu)
     const bool use_gated_activation = use_gated_activation_ && ffn_weights->intermediate_weight2.kernel != nullptr;
@@ -61,52 +61,70 @@ void FfnLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_t
     }
     else {
 #endif
-        if (int8_mode_ == 1 && m <= 2) {
-            FT_CHECK(!use_gated_activation);
-            FT_CHECK(ffn_weights->intermediate_weight.int8_kernel != NULL
-                     && ffn_weights->intermediate_weight.scale != NULL);
-            int8WeightPerChannelLdkMultiplicationLauncher(ffn_weights->intermediate_weight.int8_kernel,
-                                                          input_tensor,
-                                                          ffn_weights->intermediate_weight.scale,
-                                                          inter_buf_,
-                                                          m,
-                                                          inter_size_,
-                                                          hidden_units_,
-                                                          stream_);
+#ifdef SPARSITY_HAOJUN
+        if (ffn_weights->intermediate_weight.SplitK != 0) {
+            // FT_CHECK(!use_gated_activation);
+            cublas_wrapper_->SpMM(
+                ffn_weights->intermediate_weight.NZWeights,
+                ffn_weights->intermediate_weight.TileOffsets,
+                input_tensor,
+                inter_buf_,
+                inter_size_,
+                m,
+                hidden_units_,
+                m > 128 ? 1 : ffn_weights->intermediate_weight.SplitK  // set to 1 if N dimension is big enough
+            );
         }
         else {
-            if (int8_mode_ == 1) {
-                printf("[WARNING][FfnLayer<T>::forward] int8 gpt doesn't support m > 2, run fp gpt instead.\n");
+#endif
+            if (int8_mode_ == 1 && m <= 2) {
+                FT_CHECK(!use_gated_activation);
+                FT_CHECK(ffn_weights->intermediate_weight.int8_kernel != NULL
+                         && ffn_weights->intermediate_weight.scale != NULL);
+                int8WeightPerChannelLdkMultiplicationLauncher(ffn_weights->intermediate_weight.int8_kernel,
+                                                              input_tensor,
+                                                              ffn_weights->intermediate_weight.scale,
+                                                              inter_buf_,
+                                                              m,
+                                                              inter_size_,
+                                                              hidden_units_,
+                                                              stream_);
             }
-            cublas_wrapper_->Gemm(CUBLAS_OP_N,
-                                  CUBLAS_OP_N,
-                                  inter_size_,
-                                  m,
-                                  hidden_units_,
-                                  ffn_weights->intermediate_weight.kernel,
-                                  inter_size_,
-                                  input_tensor,
-                                  hidden_units_,
-                                  inter_buf_,
-                                  inter_size_);
-            if (use_gated_activation) {
+            else {
+                if (int8_mode_ == 1) {
+                    printf("[WARNING][FfnLayer<T>::forward] int8 gpt doesn't support m > 2, run fp gpt instead.\n");
+                }
                 cublas_wrapper_->Gemm(CUBLAS_OP_N,
                                       CUBLAS_OP_N,
                                       inter_size_,
                                       m,
                                       hidden_units_,
-                                      ffn_weights->intermediate_weight2.kernel,
+                                      ffn_weights->intermediate_weight.kernel,
                                       inter_size_,
                                       input_tensor,
                                       hidden_units_,
-                                      inter_buf_2_,
+                                      inter_buf_,
                                       inter_size_);
+                if (use_gated_activation) {
+                    cublas_wrapper_->Gemm(CUBLAS_OP_N,
+                                          CUBLAS_OP_N,
+                                          inter_size_,
+                                          m,
+                                          hidden_units_,
+                                          ffn_weights->intermediate_weight2.kernel,
+                                          inter_size_,
+                                          input_tensor,
+                                          hidden_units_,
+                                          inter_buf_2_,
+                                          inter_size_);
+                }
             }
-        }
 #ifdef SPARSITY_ENABLED
+        }
+#endif
+#ifdef SPARSITY_HAOJUN
     }
 #endif
-
     if (use_gated_activation) {
         invokeAddBiasGatedActivation(m, ffn_weights->intermediate_weight.bias, ffn_weights->intermediate_weight2.bias);
     }
@@ -128,31 +146,49 @@ void FfnLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_t
     }
     else {
 #endif
-        if (int8_mode_ == 1 && m <= 2) {
-            FT_CHECK(ffn_weights->output_weight.int8_kernel != NULL && ffn_weights->output_weight.scale != NULL);
-            int8WeightPerChannelLdkMultiplicationLauncher(ffn_weights->output_weight.int8_kernel,
-                                                          inter_buf_,
-                                                          ffn_weights->output_weight.scale,
-                                                          output_tensor,
-                                                          m,
-                                                          hidden_units_,
-                                                          inter_size_,
-                                                          stream_);
-        }
-        else {
-            cublas_wrapper_->Gemm(CUBLAS_OP_N,
-                                  CUBLAS_OP_N,
+#ifdef SPARSITY_HAOJUN
+        if (ffn_weights->output_weight.SplitK != 0) {
+            cublas_wrapper_->SpMM(ffn_weights->output_weight.NZWeights,
+                                  ffn_weights->output_weight.TileOffsets,
+                                  inter_buf_,
+                                  output_tensor,
                                   hidden_units_,
                                   m,
                                   inter_size_,
-                                  ffn_weights->output_weight.kernel,
-                                  hidden_units_,
-                                  inter_buf_,
-                                  inter_size_,
-                                  output_tensor,
-                                  hidden_units_);
+                                  m > 128 ? 1 :
+                                            ffn_weights->output_weight.SplitK  // set to 1 if N dimension is big enough
+            );
         }
+        else {
+#endif
+            if (int8_mode_ == 1 && m <= 2) {
+                FT_CHECK(ffn_weights->output_weight.int8_kernel != NULL && ffn_weights->output_weight.scale != NULL);
+                int8WeightPerChannelLdkMultiplicationLauncher(ffn_weights->output_weight.int8_kernel,
+                                                              inter_buf_,
+                                                              ffn_weights->output_weight.scale,
+                                                              output_tensor,
+                                                              m,
+                                                              hidden_units_,
+                                                              inter_size_,
+                                                              stream_);
+            }
+            else {
+                cublas_wrapper_->Gemm(CUBLAS_OP_N,
+                                      CUBLAS_OP_N,
+                                      hidden_units_,
+                                      m,
+                                      inter_size_,
+                                      ffn_weights->output_weight.kernel,
+                                      hidden_units_,
+                                      inter_buf_,
+                                      inter_size_,
+                                      output_tensor,
+                                      hidden_units_);
+            }
 #ifdef SPARSITY_ENABLED
+        }
+#endif
+#ifdef SPARSITY_HAOJUN
     }
 #endif
     sync_check_cuda_error();
@@ -163,18 +199,18 @@ void FfnLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_t
 }
 
 template<typename T>
-FfnLayer<T>::FfnLayer(size_t           max_batch_size,
-                      size_t           max_seq_len,
-                      size_t           head_num,
-                      size_t           size_per_head,
-                      size_t           inter_size,
-                      cudaStream_t     stream,
+FfnLayer<T>::FfnLayer(size_t max_batch_size,
+                      size_t max_seq_len,
+                      size_t head_num,
+                      size_t size_per_head,
+                      size_t inter_size,
+                      cudaStream_t stream,
                       cublasMMWrapper* cublas_wrapper,
-                      IAllocator*      allocator,
-                      bool             is_free_buffer_after_forward,
-                      bool             sparse,
-                      int              int8_mode,
-                      bool             use_gated_activation):
+                      IAllocator* allocator,
+                      bool is_free_buffer_after_forward,
+                      bool sparse,
+                      int int8_mode,
+                      bool use_gated_activation):
     BaseLayer(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, nullptr, sparse),
     max_token_num_(max_batch_size * max_seq_len),
     head_num_(head_num),
@@ -269,18 +305,18 @@ template class FfnLayer<__nv_bfloat16>;
 #endif
 
 template<typename T>
-GeluFfnLayer<T>::GeluFfnLayer(size_t           max_batch_size,
-                              size_t           max_seq_len,
-                              size_t           head_num,
-                              size_t           size_per_head,
-                              size_t           inter_size,
-                              cudaStream_t     stream,
+GeluFfnLayer<T>::GeluFfnLayer(size_t max_batch_size,
+                              size_t max_seq_len,
+                              size_t head_num,
+                              size_t size_per_head,
+                              size_t inter_size,
+                              cudaStream_t stream,
                               cublasMMWrapper* cublas_wrapper,
-                              IAllocator*      allocator,
-                              bool             is_free_buffer_after_forward,
-                              bool             sparse,
-                              int              int8_mode,
-                              bool             use_gated_activation):
+                              IAllocator* allocator,
+                              bool is_free_buffer_after_forward,
+                              bool sparse,
+                              int int8_mode,
+                              bool use_gated_activation):
     FfnLayer<T>(max_batch_size,
                 max_seq_len,
                 head_num,
@@ -320,17 +356,17 @@ template class GeluFfnLayer<__nv_bfloat16>;
 #endif
 
 template<typename T>
-ReluFfnLayer<T>::ReluFfnLayer(size_t           max_batch_size,
-                              size_t           max_seq_len,
-                              size_t           head_num,
-                              size_t           size_per_head,
-                              size_t           inter_size,
-                              cudaStream_t     stream,
+ReluFfnLayer<T>::ReluFfnLayer(size_t max_batch_size,
+                              size_t max_seq_len,
+                              size_t head_num,
+                              size_t size_per_head,
+                              size_t inter_size,
+                              cudaStream_t stream,
                               cublasMMWrapper* cublas_wrapper,
-                              IAllocator*      allocator,
-                              bool             is_free_buffer_after_forward,
-                              bool             sparse,
-                              bool             use_gated_activation):
+                              IAllocator* allocator,
+                              bool is_free_buffer_after_forward,
+                              bool sparse,
+                              bool use_gated_activation):
     FfnLayer<T>(max_batch_size,
                 max_seq_len,
                 head_num,
@@ -370,17 +406,17 @@ template class ReluFfnLayer<__nv_bfloat16>;
 #endif
 
 template<typename T>
-SiluFfnLayer<T>::SiluFfnLayer(size_t           max_batch_size,
-                              size_t           max_seq_len,
-                              size_t           head_num,
-                              size_t           size_per_head,
-                              size_t           inter_size,
-                              cudaStream_t     stream,
+SiluFfnLayer<T>::SiluFfnLayer(size_t max_batch_size,
+                              size_t max_seq_len,
+                              size_t head_num,
+                              size_t size_per_head,
+                              size_t inter_size,
+                              cudaStream_t stream,
                               cublasMMWrapper* cublas_wrapper,
-                              IAllocator*      allocator,
-                              bool             is_free_buffer_after_forward,
-                              bool             sparse,
-                              bool             use_gated_activation):
+                              IAllocator* allocator,
+                              bool is_free_buffer_after_forward,
+                              bool sparse,
+                              bool use_gated_activation):
     FfnLayer<T>(max_batch_size,
                 max_seq_len,
                 head_num,
diff --git a/src/fastertransformer/layers/attention_layers/DecoderSelfAttentionLayer.cc b/src/fastertransformer/layers/attention_layers/DecoderSelfAttentionLayer.cc
index bfd503d..0982773 100644
--- a/src/fastertransformer/layers/attention_layers/DecoderSelfAttentionLayer.cc
+++ b/src/fastertransformer/layers/attention_layers/DecoderSelfAttentionLayer.cc
@@ -31,31 +31,31 @@ struct SATypeConverter<half> {
 };
 
 template<typename T>
-void fusedQKV_masked_attention_dispatch(const T*     qkv_buf,
-                                        const T*     qkv_bias,
-                                        const T*     relative_attention_bias,
-                                        T*           key_cache,
-                                        T*           value_cache,
-                                        const int*   cache_indir,
-                                        T*           context_buf,
-                                        const bool*  finished,
-                                        const int*   sequence_lengths,
-                                        const int    max_batch_size,
-                                        const int    inference_batch_size,
-                                        const int    beam_width,
-                                        const int    head_num,
-                                        const int    size_per_head,
-                                        const int    rotary_embedding_dim,
-                                        const bool   neox_rotary_style,
-                                        const int    memory_max_len,
-                                        const int*   prefix_prompt_lengths,
-                                        const int    max_prefix_prompt_length,
-                                        const int    max_input_len,
-                                        const int*   total_padding_tokens,
-                                        const int    step,
-                                        const float  q_scaling,
-                                        const int    relative_attention_bias_stride,
-                                        const bool*  masked_tokens,
+void fusedQKV_masked_attention_dispatch(const T* qkv_buf,
+                                        const T* qkv_bias,
+                                        const T* relative_attention_bias,
+                                        T* key_cache,
+                                        T* value_cache,
+                                        const int* cache_indir,
+                                        T* context_buf,
+                                        const bool* finished,
+                                        const int* sequence_lengths,
+                                        const int max_batch_size,
+                                        const int inference_batch_size,
+                                        const int beam_width,
+                                        const int head_num,
+                                        const int size_per_head,
+                                        const int rotary_embedding_dim,
+                                        const bool neox_rotary_style,
+                                        const int memory_max_len,
+                                        const int* prefix_prompt_lengths,
+                                        const int max_prefix_prompt_length,
+                                        const int max_input_len,
+                                        const int* total_padding_tokens,
+                                        const int step,
+                                        const float q_scaling,
+                                        const int relative_attention_bias_stride,
+                                        const bool* masked_tokens,
                                         cudaStream_t stream)
 {
     using DataType = typename SATypeConverter<T>::Type;
@@ -78,27 +78,27 @@ void fusedQKV_masked_attention_dispatch(const T*     qkv_buf,
     params.out = reinterpret_cast<DataType*>(context_buf);
 
     // Set the input buffers.
-    params.q        = reinterpret_cast<const DataType*>(qkv_buf);
-    params.k        = reinterpret_cast<const DataType*>(qkv_buf) + hidden_units;
-    params.v        = reinterpret_cast<const DataType*>(qkv_buf) + 2 * hidden_units;
-    params.stride   = 3 * hidden_units;
+    params.q = reinterpret_cast<const DataType*>(qkv_buf);
+    params.k = reinterpret_cast<const DataType*>(qkv_buf) + hidden_units;
+    params.v = reinterpret_cast<const DataType*>(qkv_buf) + 2 * hidden_units;
+    params.stride = 3 * hidden_units;
     params.finished = const_cast<bool*>(finished);
 
-    params.k_cache                  = reinterpret_cast<DataType*>(key_cache);
-    params.v_cache                  = reinterpret_cast<DataType*>(value_cache);
-    params.cache_indir              = cache_indir;
-    params.batch_size               = inference_batch_size;
-    params.beam_width               = beam_width;
-    params.memory_max_len           = memory_max_len;
-    params.prefix_prompt_lengths    = prefix_prompt_lengths;
+    params.k_cache = reinterpret_cast<DataType*>(key_cache);
+    params.v_cache = reinterpret_cast<DataType*>(value_cache);
+    params.cache_indir = cache_indir;
+    params.batch_size = inference_batch_size;
+    params.beam_width = beam_width;
+    params.memory_max_len = memory_max_len;
+    params.prefix_prompt_lengths = prefix_prompt_lengths;
     params.max_prefix_prompt_length = max_prefix_prompt_length;
-    params.length_per_sample        = sequence_lengths;  // max_input_length + current output length
+    params.length_per_sample = sequence_lengths;  // max_input_length + current output length
     // timestep adding max_prefix_prompt_length for shared memory size calculation and rotary embedding computation
-    params.timestep             = step + max_prefix_prompt_length - 1;
-    params.num_heads            = head_num;
+    params.timestep = step + max_prefix_prompt_length - 1;
+    params.num_heads = head_num;
     params.hidden_size_per_head = size_per_head;
     params.rotary_embedding_dim = rotary_embedding_dim;
-    params.neox_rotary_style    = neox_rotary_style;
+    params.neox_rotary_style = neox_rotary_style;
     // Note: keep norm factor (sqrt(K_dim)) when adopting megatron T5 structure (may adjust)
     params.inv_sqrt_dh = 1.F / (sqrtf((float)params.hidden_size_per_head) * q_scaling);
 
@@ -107,7 +107,7 @@ void fusedQKV_masked_attention_dispatch(const T*     qkv_buf,
         params.relative_attention_bias = reinterpret_cast<const DataType*>(relative_attention_bias);
     }
     params.relative_attention_bias_stride = relative_attention_bias_stride;
-    params.masked_tokens                  = masked_tokens;
+    params.masked_tokens = masked_tokens;
 
     masked_multihead_attention(params, stream);
 }
@@ -115,55 +115,55 @@ void fusedQKV_masked_attention_dispatch(const T*     qkv_buf,
 template void fusedQKV_masked_attention_dispatch(const float* qkv_buf,
                                                  const float* qkv_bias,
                                                  const float* relative_attention_bias,
-                                                 float*       key_cache,
-                                                 float*       value_cache,
-                                                 const int*   cache_indir,
-                                                 float*       context_buf,
-                                                 const bool*  finished,
-                                                 const int*   sequence_lengths,
-                                                 const int    max_batch_size,
-                                                 const int    inference_batch_size,
-                                                 const int    beam_width,
-                                                 const int    head_num,
-                                                 const int    size_per_head,
-                                                 const int    rotary_embedding_dim,
-                                                 const bool   neox_rotary_style,
-                                                 const int    memory_max_len,
-                                                 const int*   prefix_prompt_lengths,
-                                                 const int    max_prefix_prompt_length,
-                                                 const int    max_input_len,
-                                                 const int*   total_padding_tokens,
-                                                 const int    step,
-                                                 const float  q_scaling,
-                                                 const int    relative_attention_bias_stride,
-                                                 const bool*  masked_tokens,
+                                                 float* key_cache,
+                                                 float* value_cache,
+                                                 const int* cache_indir,
+                                                 float* context_buf,
+                                                 const bool* finished,
+                                                 const int* sequence_lengths,
+                                                 const int max_batch_size,
+                                                 const int inference_batch_size,
+                                                 const int beam_width,
+                                                 const int head_num,
+                                                 const int size_per_head,
+                                                 const int rotary_embedding_dim,
+                                                 const bool neox_rotary_style,
+                                                 const int memory_max_len,
+                                                 const int* prefix_prompt_lengths,
+                                                 const int max_prefix_prompt_length,
+                                                 const int max_input_len,
+                                                 const int* total_padding_tokens,
+                                                 const int step,
+                                                 const float q_scaling,
+                                                 const int relative_attention_bias_stride,
+                                                 const bool* masked_tokens,
                                                  cudaStream_t stream);
 
-template void fusedQKV_masked_attention_dispatch(const half*  qkv_buf,
-                                                 const half*  qkv_bias,
-                                                 const half*  relative_attention_bias,
-                                                 half*        key_cache,
-                                                 half*        value_cache,
-                                                 const int*   cache_indir,
-                                                 half*        context_buf,
-                                                 const bool*  finished,
-                                                 const int*   sequence_lengths,
-                                                 const int    max_batch_size,
-                                                 const int    inference_batch_size,
-                                                 const int    beam_width,
-                                                 const int    head_num,
-                                                 const int    size_per_head,
-                                                 const int    rotary_embedding_dim,
-                                                 const bool   neox_rotary_style,
-                                                 const int    memory_max_len,
-                                                 const int*   prefix_prompt_lengths,
-                                                 const int    max_prefix_prompt_length,
-                                                 const int    max_input_len,
-                                                 const int*   total_padding_tokens,
-                                                 const int    step,
-                                                 const float  q_scaling,
-                                                 const int    relative_attention_bias_stride,
-                                                 const bool*  masked_tokens,
+template void fusedQKV_masked_attention_dispatch(const half* qkv_buf,
+                                                 const half* qkv_bias,
+                                                 const half* relative_attention_bias,
+                                                 half* key_cache,
+                                                 half* value_cache,
+                                                 const int* cache_indir,
+                                                 half* context_buf,
+                                                 const bool* finished,
+                                                 const int* sequence_lengths,
+                                                 const int max_batch_size,
+                                                 const int inference_batch_size,
+                                                 const int beam_width,
+                                                 const int head_num,
+                                                 const int size_per_head,
+                                                 const int rotary_embedding_dim,
+                                                 const bool neox_rotary_style,
+                                                 const int memory_max_len,
+                                                 const int* prefix_prompt_lengths,
+                                                 const int max_prefix_prompt_length,
+                                                 const int max_input_len,
+                                                 const int* total_padding_tokens,
+                                                 const int step,
+                                                 const float q_scaling,
+                                                 const int relative_attention_bias_stride,
+                                                 const bool* masked_tokens,
                                                  cudaStream_t stream);
 
 template<typename T>
@@ -213,20 +213,20 @@ bool DecoderSelfAttentionLayer<T>::isValidBatchSize(size_t batch_size)
 }
 
 template<typename T>
-DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_batch_size,
-                                                        size_t           head_num,
-                                                        size_t           size_per_head,
-                                                        size_t           local_head_num,
-                                                        size_t           rotary_embedding_dim,
-                                                        bool             neox_rotary_style,
-                                                        size_t           d_model,
-                                                        const float      q_scaling,
-                                                        cudaStream_t     stream,
+DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t max_batch_size,
+                                                        size_t head_num,
+                                                        size_t size_per_head,
+                                                        size_t local_head_num,
+                                                        size_t rotary_embedding_dim,
+                                                        bool neox_rotary_style,
+                                                        size_t d_model,
+                                                        const float q_scaling,
+                                                        cudaStream_t stream,
                                                         cublasMMWrapper* cublas_wrapper,
-                                                        IAllocator*      allocator,
-                                                        bool             is_free_buffer_after_forward,
-                                                        bool             sparse,
-                                                        int              int8_mode):
+                                                        IAllocator* allocator,
+                                                        bool is_free_buffer_after_forward,
+                                                        bool sparse,
+                                                        int int8_mode):
     BaseAttentionLayer<T>(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, sparse),
     max_batch_size_(max_batch_size),
     head_num_(head_num),
@@ -246,15 +246,15 @@ DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_bat
 }
 
 template<typename T>
-DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_batch_size,
-                                                        size_t           head_num,
-                                                        size_t           size_per_head,
-                                                        cudaStream_t     stream,
+DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t max_batch_size,
+                                                        size_t head_num,
+                                                        size_t size_per_head,
+                                                        cudaStream_t stream,
                                                         cublasMMWrapper* cublas_wrapper,
-                                                        IAllocator*      allocator,
-                                                        bool             is_free_buffer_after_forward,
-                                                        bool             sparse,
-                                                        int              int8_mode):
+                                                        IAllocator* allocator,
+                                                        bool is_free_buffer_after_forward,
+                                                        bool sparse,
+                                                        int int8_mode):
     DecoderSelfAttentionLayer<T>(max_batch_size,
                                  head_num,
                                  size_per_head,
@@ -273,16 +273,16 @@ DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_bat
 }
 
 template<typename T>
-DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_batch_size,
-                                                        size_t           head_num,
-                                                        size_t           size_per_head,
-                                                        const float      q_scaling,
-                                                        cudaStream_t     stream,
+DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t max_batch_size,
+                                                        size_t head_num,
+                                                        size_t size_per_head,
+                                                        const float q_scaling,
+                                                        cudaStream_t stream,
                                                         cublasMMWrapper* cublas_wrapper,
-                                                        IAllocator*      allocator,
-                                                        bool             is_free_buffer_after_forward,
-                                                        bool             sparse,
-                                                        int              int8_mode):
+                                                        IAllocator* allocator,
+                                                        bool is_free_buffer_after_forward,
+                                                        bool sparse,
+                                                        int int8_mode):
     DecoderSelfAttentionLayer<T>(max_batch_size,
                                  head_num,
                                  size_per_head,
@@ -301,16 +301,16 @@ DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_bat
 }
 
 template<typename T>
-DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_batch_size,
-                                                        size_t           head_num,
-                                                        size_t           size_per_head,
-                                                        size_t           local_head_num,
-                                                        cudaStream_t     stream,
+DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t max_batch_size,
+                                                        size_t head_num,
+                                                        size_t size_per_head,
+                                                        size_t local_head_num,
+                                                        cudaStream_t stream,
                                                         cublasMMWrapper* cublas_wrapper,
-                                                        IAllocator*      allocator,
-                                                        bool             is_free_buffer_after_forward,
-                                                        bool             sparse,
-                                                        int              int8_mode):
+                                                        IAllocator* allocator,
+                                                        bool is_free_buffer_after_forward,
+                                                        bool sparse,
+                                                        int int8_mode):
     DecoderSelfAttentionLayer<T>(max_batch_size,
                                  head_num,
                                  size_per_head,
@@ -329,18 +329,18 @@ DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_bat
 }
 
 template<typename T>
-DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_batch_size,
-                                                        size_t           head_num,
-                                                        size_t           size_per_head,
-                                                        size_t           local_head_num,
-                                                        size_t           d_model,
-                                                        const float      q_scaling,
-                                                        cudaStream_t     stream,
+DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t max_batch_size,
+                                                        size_t head_num,
+                                                        size_t size_per_head,
+                                                        size_t local_head_num,
+                                                        size_t d_model,
+                                                        const float q_scaling,
+                                                        cudaStream_t stream,
                                                         cublasMMWrapper* cublas_wrapper,
-                                                        IAllocator*      allocator,
-                                                        bool             is_free_buffer_after_forward,
-                                                        bool             sparse,
-                                                        int              int8_mode):
+                                                        IAllocator* allocator,
+                                                        bool is_free_buffer_after_forward,
+                                                        bool sparse,
+                                                        int int8_mode):
     DecoderSelfAttentionLayer<T>(max_batch_size,
                                  head_num,
                                  size_per_head,
@@ -359,18 +359,18 @@ DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_bat
 }
 
 template<typename T>
-DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t           max_batch_size,
-                                                        size_t           head_num,
-                                                        size_t           size_per_head,
-                                                        size_t           local_head_num,
-                                                        size_t           rotary_embedding_dim,
-                                                        bool             neox_rotary_style,
-                                                        cudaStream_t     stream,
+DecoderSelfAttentionLayer<T>::DecoderSelfAttentionLayer(size_t max_batch_size,
+                                                        size_t head_num,
+                                                        size_t size_per_head,
+                                                        size_t local_head_num,
+                                                        size_t rotary_embedding_dim,
+                                                        bool neox_rotary_style,
+                                                        cudaStream_t stream,
                                                         cublasMMWrapper* cublas_wrapper,
-                                                        IAllocator*      allocator,
-                                                        bool             is_free_buffer_after_forward,
-                                                        bool             sparse,
-                                                        int              int8_mode):
+                                                        IAllocator* allocator,
+                                                        bool is_free_buffer_after_forward,
+                                                        bool sparse,
+                                                        int int8_mode):
     DecoderSelfAttentionLayer<T>(max_batch_size,
                                  head_num,
                                  size_per_head,
@@ -415,9 +415,9 @@ DecoderSelfAttentionLayer<T>::~DecoderSelfAttentionLayer()
 }
 
 template<typename T>
-void DecoderSelfAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_tensors,
+void DecoderSelfAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>* output_tensors,
                                            const std::vector<fastertransformer::Tensor>* input_tensors,
-                                           const AttentionWeight<T>*                     attention_weights)
+                                           const AttentionWeight<T>* attention_weights)
 {
     // input tensors:
     //      attention_input [batch_size, d_model_],
@@ -444,24 +444,24 @@ void DecoderSelfAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor
     FT_CHECK(output_tensors->at(2).shape.size() == 4 || output_tensors->at(2).shape.size() == 3);
     allocateBuffer(input_tensors->at(0).shape[0]);
 
-    const T*    attention_input         = input_tensors->at(0).getPtr<T>();
-    const bool* finished                = input_tensors->at(1).getPtr<bool>();
-    const int*  sequence_lengths        = input_tensors->at(2).getPtr<int>();
-    const int*  cache_indir             = input_tensors->at(8).getPtr<int>();
-    const bool* masked_tokens           = input_tensors->at(9).getPtr<bool>();
-    const T*    relative_attention_bias = input_tensors->size() == 11 ? input_tensors->at(10).getPtr<T>() : nullptr;
-    const int   relative_attention_bias_stride = input_tensors->size() == 11 ? input_tensors->at(10).shape[3] : 0;
+    const T* attention_input = input_tensors->at(0).getPtr<T>();
+    const bool* finished = input_tensors->at(1).getPtr<bool>();
+    const int* sequence_lengths = input_tensors->at(2).getPtr<int>();
+    const int* cache_indir = input_tensors->at(8).getPtr<int>();
+    const bool* masked_tokens = input_tensors->at(9).getPtr<bool>();
+    const T* relative_attention_bias = input_tensors->size() == 11 ? input_tensors->at(10).getPtr<T>() : nullptr;
+    const int relative_attention_bias_stride = input_tensors->size() == 11 ? input_tensors->at(10).shape[3] : 0;
 
     T* attention_out = (T*)(output_tensors->at(0).data);
-    T* key_cache     = (T*)(output_tensors->at(1).data);
-    T* value_cache   = (T*)(output_tensors->at(2).data);
+    T* key_cache = (T*)(output_tensors->at(1).data);
+    T* value_cache = (T*)(output_tensors->at(2).data);
 
-    const int batch_size     = input_tensors->at(0).shape[0];
-    const int beam_width     = input_tensors->at(8).shape[1];
+    const int batch_size = input_tensors->at(0).shape[0];
+    const int beam_width = input_tensors->at(8).shape[1];
     const int memory_max_len = output_tensors->at(1).shape[3];
 
-    const int* d_prefix_prompt_lengths  = input_tensors->at(4).getPtr<int>();
-    const int  max_prefix_prompt_length = input_tensors->at(5).getVal<int>();
+    const int* d_prefix_prompt_lengths = input_tensors->at(4).getPtr<int>();
+    const int max_prefix_prompt_length = input_tensors->at(5).getVal<int>();
 
 #ifdef SPARSITY_ENABLED
     const int m_padded = 8 * div_up(batch_size, 8);
@@ -477,36 +477,54 @@ void DecoderSelfAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor
     }
     else {
 #endif
-        if (int8_mode_ != 0 && batch_size <= 2) {
-            FT_CHECK(attention_weights->query_weight.int8_kernel != NULL
-                     && attention_weights->query_weight.scale != NULL);
-            int8WeightPerChannelLdkMultiplicationLauncher(attention_weights->query_weight.int8_kernel,
-                                                          attention_input,
-                                                          attention_weights->query_weight.scale,
-                                                          qkv_buf_,
-                                                          batch_size,
-                                                          3 * local_hidden_units_,
-                                                          d_model_,
-                                                          stream_);
-        }
-        else {
-            if (int8_mode_ == 1) {
-                FT_LOG_WARNING(
-                    "[DecoderSelfAttentionLayer<T>::forward] int8 gpt doesn't support m > 2, run fp gpt instead.\n");
-            }
-            cublas_wrapper_->Gemm(CUBLAS_OP_N,
-                                  CUBLAS_OP_N,
-                                  3 * local_hidden_units_,  // n
-                                  batch_size,
-                                  d_model_,  // k
-                                  attention_weights->query_weight.kernel,
-                                  3 * local_hidden_units_,  // n
+#ifdef SPARSITY_HAOJUN
+        if (attention_weights->query_weight.SplitK != 0) {
+            // printf("DecoderSelfAttentionLayer.cc: calling SpMM with M:%d N:%d K:%d\n", 3 * local_hidden_units_,
+            // batch_size, d_model_);
+            cublas_wrapper_->SpMM(attention_weights->query_weight.NZWeights,
+                                  attention_weights->query_weight.TileOffsets,
                                   attention_input,
-                                  d_model_,  // k
                                   qkv_buf_,
-                                  3 * local_hidden_units_ /* n */);
+                                  3 * local_hidden_units_,
+                                  batch_size,
+                                  d_model_,
+                                  attention_weights->query_weight.SplitK);
         }
+        else {
+#endif
+            if (int8_mode_ != 0 && batch_size <= 2) {
+                FT_CHECK(attention_weights->query_weight.int8_kernel != NULL
+                         && attention_weights->query_weight.scale != NULL);
+                int8WeightPerChannelLdkMultiplicationLauncher(attention_weights->query_weight.int8_kernel,
+                                                              attention_input,
+                                                              attention_weights->query_weight.scale,
+                                                              qkv_buf_,
+                                                              batch_size,
+                                                              3 * local_hidden_units_,
+                                                              d_model_,
+                                                              stream_);
+            }
+            else {
+                if (int8_mode_ == 1) {
+                    FT_LOG_WARNING(
+                        "[DecoderSelfAttentionLayer<T>::forward] int8 gpt doesn't support m > 2, run fp gpt instead.\n");
+                }
+                cublas_wrapper_->Gemm(CUBLAS_OP_N,
+                                      CUBLAS_OP_N,
+                                      3 * local_hidden_units_,  // n
+                                      batch_size,
+                                      d_model_,  // k
+                                      attention_weights->query_weight.kernel,
+                                      3 * local_hidden_units_,  // n
+                                      attention_input,
+                                      d_model_,  // k
+                                      qkv_buf_,
+                                      3 * local_hidden_units_ /* n */);
+            }
 #ifdef SPARSITY_ENABLED
+        }
+#endif
+#ifdef SPARSITY_HAOJUN
     }
 #endif
     sync_check_cuda_error();
@@ -552,37 +570,53 @@ void DecoderSelfAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor
     }
     else {
 #endif
-        if (int8_mode_ != 0 && batch_size <= 2) {
-            FT_CHECK(attention_weights->attention_output_weight.int8_kernel != NULL
-                     && attention_weights->attention_output_weight.scale != NULL);
-            int8WeightPerChannelLdkMultiplicationLauncher(attention_weights->attention_output_weight.int8_kernel,
-                                                          context_buf_,
-                                                          attention_weights->attention_output_weight.scale,
-                                                          attention_out,
-                                                          batch_size,
-                                                          d_model_,
-                                                          local_hidden_units_,
-                                                          stream_);
-        }
-        else {
-            if (int8_mode_ == 1) {
-                FT_LOG_WARNING(
-                    "[DecoderSelfAttentionLayer<T>::forward] int8 gpt doesn't support m > 2, run fp gpt instead.\n");
-            }
-            cublas_wrapper_->Gemm(CUBLAS_OP_N,
-                                  CUBLAS_OP_N,
-                                  d_model_,  // n
-                                  batch_size,
-                                  local_hidden_units_,  // k
-                                  attention_weights->attention_output_weight.kernel,
-                                  d_model_,  // n
+#ifdef SPARSITY_HAOJUN
+        if (attention_weights->attention_output_weight.SplitK != 0) {
+            cublas_wrapper_->SpMM(attention_weights->attention_output_weight.NZWeights,
+                                  attention_weights->attention_output_weight.TileOffsets,
                                   context_buf_,
-                                  local_hidden_units_,  // k
                                   attention_out,
-                                  d_model_ /* n */);
+                                  d_model_,
+                                  batch_size,
+                                  local_hidden_units_,
+                                  attention_weights->attention_output_weight.SplitK);
         }
-        sync_check_cuda_error();
+        else {
+#endif
+            if (int8_mode_ != 0 && batch_size <= 2) {
+                FT_CHECK(attention_weights->attention_output_weight.int8_kernel != NULL
+                         && attention_weights->attention_output_weight.scale != NULL);
+                int8WeightPerChannelLdkMultiplicationLauncher(attention_weights->attention_output_weight.int8_kernel,
+                                                              context_buf_,
+                                                              attention_weights->attention_output_weight.scale,
+                                                              attention_out,
+                                                              batch_size,
+                                                              d_model_,
+                                                              local_hidden_units_,
+                                                              stream_);
+            }
+            else {
+                if (int8_mode_ == 1) {
+                    FT_LOG_WARNING(
+                        "[DecoderSelfAttentionLayer<T>::forward] int8 gpt doesn't support m > 2, run fp gpt instead.\n");
+                }
+                cublas_wrapper_->Gemm(CUBLAS_OP_N,
+                                      CUBLAS_OP_N,
+                                      d_model_,  // n
+                                      batch_size,
+                                      local_hidden_units_,  // k
+                                      attention_weights->attention_output_weight.kernel,
+                                      d_model_,  // n
+                                      context_buf_,
+                                      local_hidden_units_,  // k
+                                      attention_out,
+                                      d_model_ /* n */);
+            }
+            sync_check_cuda_error();
 #ifdef SPARSITY_ENABLED
+        }
+#endif
+#ifdef SPARSITY_HAOJUN
     }
 #endif
 
diff --git a/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc b/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc
index c628bd6..364f69f 100644
--- a/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc
+++ b/src/fastertransformer/layers/attention_layers/GptContextAttentionLayer.cc
@@ -21,9 +21,9 @@
 namespace fastertransformer {
 
 template<typename T>
-void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>*       output_tensors,
+void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>* output_tensors,
                                           const std::vector<fastertransformer::Tensor>* input_tensors,
-                                          const AttentionWeight<T>*                     attention_weights)
+                                          const AttentionWeight<T>* attention_weights)
 {
     // input_tensors:
     //      input_query [token_num, hidden_dimension]
@@ -44,21 +44,21 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
     FT_CHECK(output_tensors->size() == 3);
     FT_CHECK(output_tensors->at(1).shape.size() == 5);
     FT_CHECK(output_tensors->at(2).shape.size() == 4 || output_tensors->at(2).shape.size() == 3);
-    const int  request_batch_size      = input_tensors->at(1).shape[0];
-    const int  request_seq_len         = input_tensors->at(1).shape[2];
-    const int  max_prompt_length       = input_tensors->at(1).shape[3] - input_tensors->at(1).shape[2];
-    const int  layer_id                = *(int*)input_tensors->at(5).data;
-    const T**  d_prefix_prompt_batch   = (const T**)input_tensors->at(3).data;
+    const int request_batch_size = input_tensors->at(1).shape[0];
+    const int request_seq_len = input_tensors->at(1).shape[2];
+    const int max_prompt_length = input_tensors->at(1).shape[3] - input_tensors->at(1).shape[2];
+    const int layer_id = *(int*)input_tensors->at(5).data;
+    const T** d_prefix_prompt_batch = (const T**)input_tensors->at(3).data;
     const int* d_prefix_prompt_lengths = (const int*)input_tensors->at(4).data;
-    const int* padding_offset          = input_tensors->size() == 7 ? input_tensors->at(6).getPtr<int>() : nullptr;
+    const int* padding_offset = input_tensors->size() == 7 ? input_tensors->at(6).getPtr<int>() : nullptr;
 
     allocateBuffer(request_batch_size, request_seq_len + max_prompt_length);
     sync_check_cuda_error();
 
-    T*         attention_out   = (T*)output_tensors->at(0).data;
-    const T*   attention_input = (const T*)input_tensors->at(0).data;
-    const T*   attention_mask  = (const T*)input_tensors->at(1).data;
-    const bool is_final        = *((bool*)(input_tensors->at(2).data));
+    T* attention_out = (T*)output_tensors->at(0).data;
+    const T* attention_input = (const T*)input_tensors->at(0).data;
+    const T* attention_mask = (const T*)input_tensors->at(1).data;
+    const bool is_final = *((bool*)(input_tensors->at(2).data));
 
     const int m = input_tensors->at(0).shape[0];
 
@@ -76,21 +76,38 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
     }
     else {
 #endif
-        cublas_wrapper_->Gemm(CUBLAS_OP_N,
-                              CUBLAS_OP_N,
-                              3 * local_hidden_units_,  // n
-                              m,
-                              hidden_units_,  // k
-                              attention_weights->query_weight.kernel,
-                              3 * local_hidden_units_,  // n
-                              attention_input,
-                              hidden_units_,  // k
-                              qkv_buf_,
-                              3 * local_hidden_units_ /* n */);
+#ifdef SPARSITY_HAOJUN
+        if (attention_weights->query_weight.SplitK != 0) {
+            cublas_wrapper_->SpMM(
+                attention_weights->query_weight.NZWeights,
+                attention_weights->query_weight.TileOffsets,
+                attention_input,
+                qkv_buf_,
+                3 * local_hidden_units_,
+                m,
+                hidden_units_,
+                m > 64 ? 1 : attention_weights->query_weight.SplitK  // set to 1 if N dimension is big enough
+            );
+        }
+        else {
+#endif
+            cublas_wrapper_->Gemm(CUBLAS_OP_N,
+                                  CUBLAS_OP_N,
+                                  3 * local_hidden_units_,  // n
+                                  m,
+                                  hidden_units_,  // k
+                                  attention_weights->query_weight.kernel,
+                                  3 * local_hidden_units_,  // n
+                                  attention_input,
+                                  hidden_units_,  // k
+                                  qkv_buf_,
+                                  3 * local_hidden_units_ /* n */);
 #ifdef SPARSITY_ENABLED
+        }
+#endif
+#ifdef SPARSITY_HAOJUN
     }
 #endif
-
     sync_check_cuda_error();
 
     // IDEA: append prefix prompt key value here
@@ -142,9 +159,9 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
     // NOTE: qkv buffer shape (batch_size, num_heads,L or prompt_len + L, Dh)
 
     if (is_final == false) {
-        const cudaDataType_t gemm_data_type      = getCudaDataType<T>();
-        const int            attention_seq_len_1 = request_seq_len;                      // q length
-        const int            attention_seq_len_2 = max_prompt_length + request_seq_len;  // kv length
+        const cudaDataType_t gemm_data_type = getCudaDataType<T>();
+        const int attention_seq_len_1 = request_seq_len;                      // q length
+        const int attention_seq_len_2 = max_prompt_length + request_seq_len;  // kv length
         if (is_qk_buf_float_ == true && gemm_data_type != CUDA_R_32F) {
             cublas_wrapper_->stridedBatchedGemm(CUBLAS_OP_T,
                                                 CUBLAS_OP_N,
@@ -266,19 +283,38 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
         }
         else {
 #endif
-            cublas_wrapper_->Gemm(CUBLAS_OP_N,
-                                  CUBLAS_OP_N,
-                                  hidden_units_,
-                                  m,
-                                  local_hidden_units_,
-                                  attention_weights->attention_output_weight.kernel,
-                                  hidden_units_,
-                                  qkv_buf_3_,
-                                  local_hidden_units_,
-                                  attention_out,
-                                  hidden_units_);
+#ifdef SPARSITY_HAOJUN
+            if (attention_weights->attention_output_weight.SplitK != 0) {
+                cublas_wrapper_->SpMM(
+                    attention_weights->attention_output_weight.NZWeights,
+                    attention_weights->attention_output_weight.TileOffsets,
+                    qkv_buf_3_,
+                    attention_out,
+                    hidden_units_,
+                    m,
+                    local_hidden_units_,
+                    m > 64 ? 1 :
+                             attention_weights->attention_output_weight.SplitK  // set to 1 if N dimension is big enough
+                );
+            }
+            else {
+#endif
+                cublas_wrapper_->Gemm(CUBLAS_OP_N,
+                                      CUBLAS_OP_N,
+                                      hidden_units_,
+                                      m,
+                                      local_hidden_units_,
+                                      attention_weights->attention_output_weight.kernel,
+                                      hidden_units_,
+                                      qkv_buf_3_,
+                                      local_hidden_units_,
+                                      attention_out,
+                                      hidden_units_);
 
 #ifdef SPARSITY_ENABLED
+            }
+#endif
+#ifdef SPARSITY_HAOJUN
         }
 #endif
     }
@@ -290,16 +326,16 @@ void GptContextAttentionLayer<T>::forward(std::vector<fastertransformer::Tensor>
 }
 
 template<typename T>
-GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t           max_batch_size,
-                                                      size_t           max_seq_len,
-                                                      size_t           head_num,
-                                                      size_t           size_per_head,
-                                                      cudaStream_t     stream,
+GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t max_batch_size,
+                                                      size_t max_seq_len,
+                                                      size_t head_num,
+                                                      size_t size_per_head,
+                                                      cudaStream_t stream,
                                                       cublasMMWrapper* cublas_wrapper,
-                                                      IAllocator*      allocator,
-                                                      bool             is_free_buffer_after_forward,
-                                                      bool             is_qk_buf_float,
-                                                      bool             sparse):
+                                                      IAllocator* allocator,
+                                                      bool is_free_buffer_after_forward,
+                                                      bool is_qk_buf_float,
+                                                      bool sparse):
     BaseAttentionLayer<T>(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, sparse),
     max_batch_size_(max_batch_size),
     max_seq_len_(max_seq_len),
@@ -315,17 +351,17 @@ GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t           max_batch
 }
 
 template<typename T>
-GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t           max_batch_size,
-                                                      size_t           max_seq_len,
-                                                      size_t           head_num,
-                                                      size_t           size_per_head,
-                                                      size_t           local_head_num,
-                                                      cudaStream_t     stream,
+GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t max_batch_size,
+                                                      size_t max_seq_len,
+                                                      size_t head_num,
+                                                      size_t size_per_head,
+                                                      size_t local_head_num,
+                                                      cudaStream_t stream,
                                                       cublasMMWrapper* cublas_wrapper,
-                                                      IAllocator*      allocator,
-                                                      bool             is_free_buffer_after_forward,
-                                                      bool             is_qk_buf_float,
-                                                      bool             sparse):
+                                                      IAllocator* allocator,
+                                                      bool is_free_buffer_after_forward,
+                                                      bool is_qk_buf_float,
+                                                      bool sparse):
     BaseAttentionLayer<T>(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, sparse),
     max_batch_size_(max_batch_size),
     max_seq_len_(max_seq_len),
@@ -341,19 +377,19 @@ GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t           max_batch
 }
 
 template<typename T>
-GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t           max_batch_size,
-                                                      size_t           max_seq_len,
-                                                      size_t           head_num,
-                                                      size_t           size_per_head,
-                                                      size_t           local_head_num,
-                                                      size_t           rotary_embedding_dim,
-                                                      bool             neox_rotary_style,
-                                                      cudaStream_t     stream,
+GptContextAttentionLayer<T>::GptContextAttentionLayer(size_t max_batch_size,
+                                                      size_t max_seq_len,
+                                                      size_t head_num,
+                                                      size_t size_per_head,
+                                                      size_t local_head_num,
+                                                      size_t rotary_embedding_dim,
+                                                      bool neox_rotary_style,
+                                                      cudaStream_t stream,
                                                       cublasMMWrapper* cublas_wrapper,
-                                                      IAllocator*      allocator,
-                                                      bool             is_free_buffer_after_forward,
-                                                      bool             is_qk_buf_float,
-                                                      bool             sparse):
+                                                      IAllocator* allocator,
+                                                      bool is_free_buffer_after_forward,
+                                                      bool is_qk_buf_float,
+                                                      bool sparse):
     BaseAttentionLayer<T>(stream, cublas_wrapper, allocator, is_free_buffer_after_forward, sparse),
     max_batch_size_(max_batch_size),
     max_seq_len_(max_seq_len),
@@ -410,7 +446,7 @@ void GptContextAttentionLayer<T>::allocateBuffer(size_t batch_size, size_t seq_l
     k_buf_2_ = q_buf_2_ + batch_size * seq_len * local_hidden_units_;
     v_buf_2_ = k_buf_2_ + batch_size * seq_len * local_hidden_units_;
 
-    qk_buf_    = (T*)allocator_->reMalloc(qk_buf_, sizeof(T) * batch_size * local_head_num_ * seq_len * seq_len, true);
+    qk_buf_ = (T*)allocator_->reMalloc(qk_buf_, sizeof(T) * batch_size * local_head_num_ * seq_len * seq_len, true);
     qkv_buf_2_ = (T*)allocator_->reMalloc(qkv_buf_2_, sizeof(T) * batch_size * seq_len * local_hidden_units_, true);
     qkv_buf_3_ = (T*)allocator_->reMalloc(qkv_buf_3_, sizeof(T) * batch_size * seq_len * local_hidden_units_, true);
 
diff --git a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.cc b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.cc
index bc98383..e1b7409 100644
--- a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.cc
+++ b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.cc
@@ -20,11 +20,11 @@
 namespace fastertransformer {
 
 template<typename T>
-ParallelGptDecoderLayerWeight<T>::ParallelGptDecoderLayerWeight(const int        hidden_units,
-                                                                const int        inter_size,
-                                                                const int        tensor_para_size,
-                                                                const int        tensor_para_rank,
-                                                                const int        int8_mode,
+ParallelGptDecoderLayerWeight<T>::ParallelGptDecoderLayerWeight(const int hidden_units,
+                                                                const int inter_size,
+                                                                const int tensor_para_size,
+                                                                const int tensor_para_rank,
+                                                                const int int8_mode,
                                                                 gptVariantParams gpt_variant_params):
     hidden_units_(hidden_units),
     inter_size_(inter_size),
@@ -40,6 +40,39 @@ ParallelGptDecoderLayerWeight<T>::ParallelGptDecoderLayerWeight(const int
     }
 }
 
+#ifdef SPARSITY_HAOJUN
+template<typename T>
+ParallelGptDecoderLayerWeight<T>::ParallelGptDecoderLayerWeight(const int hidden_units,
+                                                                const int inter_size,
+                                                                const int tensor_para_size,
+                                                                const int tensor_para_rank,
+                                                                // Rewrite construction function
+                                                                // Adding 2 new inputs for construction function
+                                                                // 1. vector<int> NNZ_List
+                                                                // 2. vector<int> SplitK_List
+                                                                const std::vector<int> NNZ_List,
+                                                                const std::vector<int> NumOffsets_List,
+                                                                const std::vector<int> SplitK_List,
+                                                                const int int8_mode,
+                                                                gptVariantParams gpt_variant_params):
+    hidden_units_(hidden_units),
+    inter_size_(inter_size),
+    tensor_para_size_(tensor_para_size),
+    tensor_para_rank_(tensor_para_rank),
+    NNZ_List_(NNZ_List),
+    NumOffsets_List_(NumOffsets_List),
+    SplitK_List_(SplitK_List),
+    int8_mode_(int8_mode),
+    gpt_variant_params_(gpt_variant_params)
+{
+    mallocWeights();
+    setWeightPtr();
+    if (int8_mode_ != 0) {
+        transposeCalibrateQuantizeWeight();
+    }
+}
+#endif
+
 template<typename T>
 ParallelGptDecoderLayerWeight<T>::ParallelGptDecoderLayerWeight(const int int8_mode): int8_mode_(int8_mode)
 {
@@ -55,29 +88,67 @@ ParallelGptDecoderLayerWeight<T>::~ParallelGptDecoderLayerWeight()
             }
         }
 
-        pre_layernorm_weights.beta                            = nullptr;
-        pre_layernorm_weights.gamma                           = nullptr;
-        self_attention_weights.query_weight.kernel            = nullptr;
-        self_attention_weights.query_weight.bias              = nullptr;
+        pre_layernorm_weights.beta = nullptr;
+        pre_layernorm_weights.gamma = nullptr;
+        self_attention_weights.query_weight.kernel = nullptr;
+        self_attention_weights.query_weight.bias = nullptr;
         self_attention_weights.attention_output_weight.kernel = nullptr;
-        self_attention_weights.attention_output_weight.bias   = nullptr;
-        self_attn_layernorm_weights.beta                      = nullptr;
-        self_attn_layernorm_weights.gamma                     = nullptr;
+        self_attention_weights.attention_output_weight.bias = nullptr;
+        self_attn_layernorm_weights.beta = nullptr;
+        self_attn_layernorm_weights.gamma = nullptr;
 
         ffn_weights.intermediate_weight.kernel = nullptr;
-        ffn_weights.intermediate_weight.bias   = nullptr;
-        ffn_weights.output_weight.kernel       = nullptr;
-        ffn_weights.output_weight.bias         = nullptr;
+        ffn_weights.intermediate_weight.bias = nullptr;
+        ffn_weights.output_weight.kernel = nullptr;
+        ffn_weights.output_weight.bias = nullptr;
 
         after_attention_adapter_weights.intermediate_weight.kernel = nullptr;
-        after_attention_adapter_weights.intermediate_weight.bias   = nullptr;
-        after_attention_adapter_weights.output_weight.kernel       = nullptr;
-        after_attention_adapter_weights.output_weight.bias         = nullptr;
+        after_attention_adapter_weights.intermediate_weight.bias = nullptr;
+        after_attention_adapter_weights.output_weight.kernel = nullptr;
+        after_attention_adapter_weights.output_weight.bias = nullptr;
 
         after_ffn_adapter_weights.intermediate_weight.kernel = nullptr;
-        after_ffn_adapter_weights.intermediate_weight.bias   = nullptr;
-        after_ffn_adapter_weights.output_weight.kernel       = nullptr;
-        after_ffn_adapter_weights.output_weight.bias         = nullptr;
+        after_ffn_adapter_weights.intermediate_weight.bias = nullptr;
+        after_ffn_adapter_weights.output_weight.kernel = nullptr;
+        after_ffn_adapter_weights.output_weight.bias = nullptr;
+
+#ifdef SPARSITY_HAOJUN
+        for (int i = 0; i < weights_ptr_NZ.size(); i++) {
+            if (weights_ptr_NZ[i] != nullptr) {
+                deviceFree(weights_ptr_NZ[i]);
+            }
+        }
+        for (int i = 0; i < weights_ptr_Offset.size(); i++) {
+            if (weights_ptr_Offset[i] != nullptr) {
+                deviceFree(weights_ptr_Offset[i]);
+            }
+        }
+        // NNZ
+        self_attention_weights.query_weight.NNZ = -1;
+        self_attention_weights.attention_output_weight.NNZ = -1;
+        ffn_weights.intermediate_weight.NNZ = -1;
+        ffn_weights.output_weight.NNZ = -1;
+        // NumOffsets
+        self_attention_weights.query_weight.NumOffsets = -1;
+        self_attention_weights.attention_output_weight.NumOffsets = -1;
+        ffn_weights.intermediate_weight.NumOffsets = -1;
+        ffn_weights.output_weight.NumOffsets = -1;
+        // SplitK
+        self_attention_weights.query_weight.SplitK = -1;
+        self_attention_weights.attention_output_weight.SplitK = -1;
+        ffn_weights.intermediate_weight.SplitK = -1;
+        ffn_weights.output_weight.SplitK = -1;
+        // NZWeights
+        self_attention_weights.query_weight.NZWeights = nullptr;
+        self_attention_weights.attention_output_weight.NZWeights = nullptr;
+        ffn_weights.intermediate_weight.NZWeights = nullptr;
+        ffn_weights.output_weight.NZWeights = nullptr;
+        // TileOffsets
+        self_attention_weights.query_weight.TileOffsets = nullptr;
+        self_attention_weights.attention_output_weight.TileOffsets = nullptr;
+        ffn_weights.intermediate_weight.TileOffsets = nullptr;
+        ffn_weights.output_weight.TileOffsets = nullptr;
+#endif
 
         if (int8_mode_ != 0) {
             for (int i = 0; i < int8_weights_ptr.size(); i++) {
@@ -90,22 +161,22 @@ ParallelGptDecoderLayerWeight<T>::~ParallelGptDecoderLayerWeight()
                     deviceFree(scale_ptr[i]);
                 }
             }
-            self_attention_weights.query_weight.int8_kernel                 = nullptr;
-            self_attention_weights.query_weight.scale                       = nullptr;
-            self_attention_weights.attention_output_weight.int8_kernel      = nullptr;
-            self_attention_weights.attention_output_weight.scale            = nullptr;
-            ffn_weights.intermediate_weight.int8_kernel                     = nullptr;
-            ffn_weights.intermediate_weight.scale                           = nullptr;
-            ffn_weights.output_weight.int8_kernel                           = nullptr;
-            ffn_weights.output_weight.scale                                 = nullptr;
+            self_attention_weights.query_weight.int8_kernel = nullptr;
+            self_attention_weights.query_weight.scale = nullptr;
+            self_attention_weights.attention_output_weight.int8_kernel = nullptr;
+            self_attention_weights.attention_output_weight.scale = nullptr;
+            ffn_weights.intermediate_weight.int8_kernel = nullptr;
+            ffn_weights.intermediate_weight.scale = nullptr;
+            ffn_weights.output_weight.int8_kernel = nullptr;
+            ffn_weights.output_weight.scale = nullptr;
             after_attention_adapter_weights.intermediate_weight.int8_kernel = nullptr;
-            after_attention_adapter_weights.intermediate_weight.scale       = nullptr;
-            after_attention_adapter_weights.output_weight.int8_kernel       = nullptr;
-            after_attention_adapter_weights.output_weight.scale             = nullptr;
-            after_ffn_adapter_weights.intermediate_weight.int8_kernel       = nullptr;
-            after_ffn_adapter_weights.intermediate_weight.scale             = nullptr;
-            after_ffn_adapter_weights.output_weight.int8_kernel             = nullptr;
-            after_ffn_adapter_weights.output_weight.scale                   = nullptr;
+            after_attention_adapter_weights.intermediate_weight.scale = nullptr;
+            after_attention_adapter_weights.output_weight.int8_kernel = nullptr;
+            after_attention_adapter_weights.output_weight.scale = nullptr;
+            after_ffn_adapter_weights.intermediate_weight.int8_kernel = nullptr;
+            after_ffn_adapter_weights.intermediate_weight.scale = nullptr;
+            after_ffn_adapter_weights.output_weight.int8_kernel = nullptr;
+            after_ffn_adapter_weights.output_weight.scale = nullptr;
         }
 
         is_maintain_buffer = false;
@@ -121,6 +192,11 @@ ParallelGptDecoderLayerWeight<T>::ParallelGptDecoderLayerWeight(const ParallelGp
     int8_mode_(other.int8_mode_),
     gpt_variant_params_(other.gpt_variant_params_)
 {
+#ifdef SPARSITY_HAOJUN
+    printf("The copy construction function for ParallelGptDecoderLayerWeight<T> is called.\n ");
+    printf("However, this function is not implemented yet!\n");
+    exit(-1);
+#endif
     mallocWeights();
     cudaD2Dcpy(weights_ptr[0], other.weights_ptr[0], hidden_units_);
     cudaD2Dcpy(weights_ptr[1], other.weights_ptr[1], hidden_units_);
@@ -192,11 +268,16 @@ template<typename T>
 ParallelGptDecoderLayerWeight<T>&
 ParallelGptDecoderLayerWeight<T>::operator=(const ParallelGptDecoderLayerWeight& other)
 {
-    hidden_units_       = other.hidden_units_;
-    inter_size_         = other.inter_size_;
-    tensor_para_size_   = other.tensor_para_size_;
-    tensor_para_rank_   = other.tensor_para_rank_;
-    int8_mode_          = other.int8_mode_;
+#ifdef SPARSITY_HAOJUN
+    printf("The copy construction function (operator=) for ParallelGptDecoderLayerWeight<T> is called.\n ");
+    printf("However, this function is not implemented yet!\n");
+    exit(-1);
+#endif
+    hidden_units_ = other.hidden_units_;
+    inter_size_ = other.inter_size_;
+    tensor_para_size_ = other.tensor_para_size_;
+    tensor_para_rank_ = other.tensor_para_rank_;
+    int8_mode_ = other.int8_mode_;
     gpt_variant_params_ = other.gpt_variant_params_;
 
     mallocWeights();
@@ -274,36 +355,98 @@ void ParallelGptDecoderLayerWeight<T>::loadModel(std::string dir_path, FtCudaDat
 
     loadWeightFromBin<T>(weights_ptr[0], {hidden_units_}, dir_path + ".input_layernorm.bias.bin", model_file_type);
     loadWeightFromBin<T>(weights_ptr[1], {hidden_units_}, dir_path + ".input_layernorm.weight.bin", model_file_type);
-    loadWeightFromBin<T>(weights_ptr[2],
-                         {hidden_units_, 3 * hidden_units_ / tensor_para_size_},
-                         dir_path + ".attention.query_key_value.weight." + std::to_string(tensor_para_rank_) + ".bin",
-                         model_file_type);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[0] != 0) {
+        loadDataArrayFromBin<unsigned int>(weights_ptr_NZ[0],
+                                           NNZ_List_[0],
+                                           dir_path + ".attention.query_key_value.weight."
+                                               + std::to_string(tensor_para_rank_) + ".NZWeights.bin");
+        loadDataArrayFromBin<int>(weights_ptr_Offset[0],
+                                  NumOffsets_List_[0],
+                                  dir_path + ".attention.query_key_value.weight." + std::to_string(tensor_para_rank_)
+                                      + ".TileOffsets.bin");
+    }
+    else
+    // printf("Printing weights_ptr_Offset[0][i]...\n");
+    // int *tmp_intArray_ptr = (int*)malloc(10*sizeof(int));
+    // cudaMemcpy(tmp_intArray_ptr, weights_ptr_Offset[0], sizeof(int) *10, cudaMemcpyDeviceToHost);
+    // for(int i=0; i<10; i++)
+    //    printf("%d ", tmp_intArray_ptr[i] );
+    // printf("\n");
+#endif
+        loadWeightFromBin<T>(weights_ptr[2],
+                             {hidden_units_, 3 * hidden_units_ / tensor_para_size_},
+                             dir_path + ".attention.query_key_value.weight." + std::to_string(tensor_para_rank_)
+                                 + ".bin",
+                             model_file_type);
+
     loadWeightFromBin<T>(weights_ptr[3],
                          {3, hidden_units_ / tensor_para_size_},
                          dir_path + ".attention.query_key_value.bias." + std::to_string(tensor_para_rank_) + ".bin",
                          model_file_type);
-    loadWeightFromBin<T>(weights_ptr[4],
-                         {hidden_units_ / tensor_para_size_, hidden_units_},
-                         dir_path + ".attention.dense.weight." + std::to_string(tensor_para_rank_) + ".bin",
-                         model_file_type);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[1] != 0) {
+        loadDataArrayFromBin<unsigned int>(weights_ptr_NZ[1],
+                                           NNZ_List_[1],
+                                           dir_path + ".attention.dense.weight." + std::to_string(tensor_para_rank_)
+                                               + ".NZWeights.bin");
+        loadDataArrayFromBin<int>(weights_ptr_Offset[1],
+                                  NumOffsets_List_[1],
+                                  dir_path + ".attention.dense.weight." + std::to_string(tensor_para_rank_)
+                                      + ".TileOffsets.bin");
+    }
+    else
+#endif
+        loadWeightFromBin<T>(weights_ptr[4],
+                             {hidden_units_ / tensor_para_size_, hidden_units_},
+                             dir_path + ".attention.dense.weight." + std::to_string(tensor_para_rank_) + ".bin",
+                             model_file_type);
+
     loadWeightFromBin<T>(weights_ptr[5], {hidden_units_}, dir_path + ".attention.dense.bias.bin", model_file_type);
     loadWeightFromBin<T>(
         weights_ptr[6], {hidden_units_}, dir_path + ".post_attention_layernorm.bias.bin", model_file_type);
     loadWeightFromBin<T>(
         weights_ptr[7], {hidden_units_}, dir_path + ".post_attention_layernorm.weight.bin", model_file_type);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[2] != 0) {
+        loadDataArrayFromBin<unsigned int>(weights_ptr_NZ[2],
+                                           NNZ_List_[2],
+                                           dir_path + ".mlp.dense_h_to_4h.weight." + std::to_string(tensor_para_rank_)
+                                               + ".NZWeights.bin");
+        loadDataArrayFromBin<int>(weights_ptr_Offset[2],
+                                  NumOffsets_List_[2],
+                                  dir_path + ".mlp.dense_h_to_4h.weight." + std::to_string(tensor_para_rank_)
+                                      + ".TileOffsets.bin");
+    }
+    else
+#endif
+        loadWeightFromBin<T>(weights_ptr[8],
+                             {hidden_units_, inter_size_ / tensor_para_size_},
+                             dir_path + ".mlp.dense_h_to_4h.weight." + std::to_string(tensor_para_rank_) + ".bin",
+                             model_file_type);
 
-    loadWeightFromBin<T>(weights_ptr[8],
-                         {hidden_units_, inter_size_ / tensor_para_size_},
-                         dir_path + ".mlp.dense_h_to_4h.weight." + std::to_string(tensor_para_rank_) + ".bin",
-                         model_file_type);
     loadWeightFromBin<T>(weights_ptr[9],
                          {inter_size_ / tensor_para_size_},
                          dir_path + ".mlp.dense_h_to_4h.bias." + std::to_string(tensor_para_rank_) + ".bin",
                          model_file_type);
-    loadWeightFromBin<T>(weights_ptr[10],
-                         {inter_size_ / tensor_para_size_, hidden_units_},
-                         dir_path + ".mlp.dense_4h_to_h.weight." + std::to_string(tensor_para_rank_) + ".bin",
-                         model_file_type);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[3] != 0) {
+        loadDataArrayFromBin<unsigned int>(weights_ptr_NZ[3],
+                                           NNZ_List_[3],
+                                           dir_path + ".mlp.dense_4h_to_h.weight." + std::to_string(tensor_para_rank_)
+                                               + ".NZWeights.bin");
+        loadDataArrayFromBin<int>(weights_ptr_Offset[3],
+                                  NumOffsets_List_[3],
+                                  dir_path + ".mlp.dense_4h_to_h.weight." + std::to_string(tensor_para_rank_)
+                                      + ".TileOffsets.bin");
+    }
+    else
+#endif
+        loadWeightFromBin<T>(weights_ptr[10],
+                             {inter_size_ / tensor_para_size_, hidden_units_},
+                             dir_path + ".mlp.dense_4h_to_h.weight." + std::to_string(tensor_para_rank_) + ".bin",
+                             model_file_type);
+
     loadWeightFromBin<T>(weights_ptr[11], {hidden_units_}, dir_path + ".mlp.dense_4h_to_h.bias.bin", model_file_type);
 
     if (gpt_variant_params_.has_adapters) {
@@ -353,49 +496,78 @@ void ParallelGptDecoderLayerWeight<T>::loadModel(std::string dir_path, FtCudaDat
 template<typename T>
 void ParallelGptDecoderLayerWeight<T>::setWeightPtr()
 {
-    pre_layernorm_weights.beta                            = weights_ptr[0];
-    pre_layernorm_weights.gamma                           = weights_ptr[1];
-    self_attention_weights.query_weight.kernel            = weights_ptr[2];
-    self_attention_weights.query_weight.bias              = weights_ptr[3];
+    pre_layernorm_weights.beta = weights_ptr[0];
+    pre_layernorm_weights.gamma = weights_ptr[1];
+    self_attention_weights.query_weight.kernel = weights_ptr[2];
+    self_attention_weights.query_weight.bias = weights_ptr[3];
     self_attention_weights.attention_output_weight.kernel = weights_ptr[4];
-    self_attention_weights.attention_output_weight.bias   = weights_ptr[5];
-    self_attn_layernorm_weights.beta                      = weights_ptr[6];
-    self_attn_layernorm_weights.gamma                     = weights_ptr[7];
+    self_attention_weights.attention_output_weight.bias = weights_ptr[5];
+    self_attn_layernorm_weights.beta = weights_ptr[6];
+    self_attn_layernorm_weights.gamma = weights_ptr[7];
 
     ffn_weights.intermediate_weight.kernel = weights_ptr[8];
-    ffn_weights.intermediate_weight.bias   = weights_ptr[9];
-    ffn_weights.output_weight.kernel       = weights_ptr[10];
-    ffn_weights.output_weight.bias         = weights_ptr[11];
+    ffn_weights.intermediate_weight.bias = weights_ptr[9];
+    ffn_weights.output_weight.kernel = weights_ptr[10];
+    ffn_weights.output_weight.bias = weights_ptr[11];
 
     after_attention_adapter_weights.intermediate_weight.kernel = weights_ptr[12];
-    after_attention_adapter_weights.intermediate_weight.bias   = weights_ptr[13];
-    after_attention_adapter_weights.output_weight.kernel       = weights_ptr[14];
-    after_attention_adapter_weights.output_weight.bias         = weights_ptr[15];
+    after_attention_adapter_weights.intermediate_weight.bias = weights_ptr[13];
+    after_attention_adapter_weights.output_weight.kernel = weights_ptr[14];
+    after_attention_adapter_weights.output_weight.bias = weights_ptr[15];
 
     after_ffn_adapter_weights.intermediate_weight.kernel = weights_ptr[16];
-    after_ffn_adapter_weights.intermediate_weight.bias   = weights_ptr[17];
-    after_ffn_adapter_weights.output_weight.kernel       = weights_ptr[18];
-    after_ffn_adapter_weights.output_weight.bias         = weights_ptr[19];
+    after_ffn_adapter_weights.intermediate_weight.bias = weights_ptr[17];
+    after_ffn_adapter_weights.output_weight.kernel = weights_ptr[18];
+    after_ffn_adapter_weights.output_weight.bias = weights_ptr[19];
 
     if (int8_mode_ != 0) {
-        self_attention_weights.query_weight.int8_kernel                 = int8_weights_ptr[0];
-        self_attention_weights.query_weight.scale                       = scale_ptr[0];
-        self_attention_weights.attention_output_weight.int8_kernel      = int8_weights_ptr[1];
-        self_attention_weights.attention_output_weight.scale            = scale_ptr[1];
-        ffn_weights.intermediate_weight.int8_kernel                     = int8_weights_ptr[2];
-        ffn_weights.intermediate_weight.scale                           = scale_ptr[2];
-        ffn_weights.output_weight.int8_kernel                           = int8_weights_ptr[3];
-        ffn_weights.output_weight.scale                                 = scale_ptr[3];
+        self_attention_weights.query_weight.int8_kernel = int8_weights_ptr[0];
+        self_attention_weights.query_weight.scale = scale_ptr[0];
+        self_attention_weights.attention_output_weight.int8_kernel = int8_weights_ptr[1];
+        self_attention_weights.attention_output_weight.scale = scale_ptr[1];
+        ffn_weights.intermediate_weight.int8_kernel = int8_weights_ptr[2];
+        ffn_weights.intermediate_weight.scale = scale_ptr[2];
+        ffn_weights.output_weight.int8_kernel = int8_weights_ptr[3];
+        ffn_weights.output_weight.scale = scale_ptr[3];
         after_attention_adapter_weights.intermediate_weight.int8_kernel = int8_weights_ptr[4];
-        after_attention_adapter_weights.intermediate_weight.scale       = scale_ptr[4];
-        after_attention_adapter_weights.output_weight.int8_kernel       = int8_weights_ptr[5];
-        after_attention_adapter_weights.output_weight.scale             = scale_ptr[5];
-        after_ffn_adapter_weights.intermediate_weight.int8_kernel       = int8_weights_ptr[6];
-        after_ffn_adapter_weights.intermediate_weight.scale             = scale_ptr[6];
-        after_ffn_adapter_weights.output_weight.int8_kernel             = int8_weights_ptr[7];
-        after_ffn_adapter_weights.output_weight.scale                   = scale_ptr[7];
+        after_attention_adapter_weights.intermediate_weight.scale = scale_ptr[4];
+        after_attention_adapter_weights.output_weight.int8_kernel = int8_weights_ptr[5];
+        after_attention_adapter_weights.output_weight.scale = scale_ptr[5];
+        after_ffn_adapter_weights.intermediate_weight.int8_kernel = int8_weights_ptr[6];
+        after_ffn_adapter_weights.intermediate_weight.scale = scale_ptr[6];
+        after_ffn_adapter_weights.output_weight.int8_kernel = int8_weights_ptr[7];
+        after_ffn_adapter_weights.output_weight.scale = scale_ptr[7];
     }
 
+#ifdef SPARSITY_HAOJUN
+    // setup the pointers and values in struct DenseWeight
+    // NNZ
+    self_attention_weights.query_weight.NNZ = NNZ_List_[0];
+    self_attention_weights.attention_output_weight.NNZ = NNZ_List_[1];
+    ffn_weights.intermediate_weight.NNZ = NNZ_List_[2];
+    ffn_weights.output_weight.NNZ = NNZ_List_[3];
+    // NumOffsets
+    self_attention_weights.query_weight.NumOffsets = NumOffsets_List_[0];
+    self_attention_weights.attention_output_weight.NumOffsets = NumOffsets_List_[1];
+    ffn_weights.intermediate_weight.NumOffsets = NumOffsets_List_[2];
+    ffn_weights.output_weight.NumOffsets = NumOffsets_List_[3];
+    // SplitK
+    self_attention_weights.query_weight.SplitK = SplitK_List_[0];
+    self_attention_weights.attention_output_weight.SplitK = SplitK_List_[1];
+    ffn_weights.intermediate_weight.SplitK = SplitK_List_[2];
+    ffn_weights.output_weight.SplitK = SplitK_List_[3];
+    // NZWeights
+    self_attention_weights.query_weight.NZWeights = weights_ptr_NZ[0];
+    self_attention_weights.attention_output_weight.NZWeights = weights_ptr_NZ[1];
+    ffn_weights.intermediate_weight.NZWeights = weights_ptr_NZ[2];
+    ffn_weights.output_weight.NZWeights = weights_ptr_NZ[3];
+    // TileOffsets
+    self_attention_weights.query_weight.TileOffsets = weights_ptr_Offset[0];
+    self_attention_weights.attention_output_weight.TileOffsets = weights_ptr_Offset[1];
+    ffn_weights.intermediate_weight.TileOffsets = weights_ptr_Offset[2];
+    ffn_weights.output_weight.TileOffsets = weights_ptr_Offset[3];
+#endif
+
     is_maintain_buffer = true;
 }
 
@@ -404,18 +576,45 @@ void ParallelGptDecoderLayerWeight<T>::mallocWeights()
 {
     deviceMalloc(&weights_ptr[0], hidden_units_);
     deviceMalloc(&weights_ptr[1], hidden_units_);
-    deviceMalloc(&weights_ptr[2], hidden_units_ * 3 * hidden_units_ / tensor_para_size_);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[0] == 0)  // Do not malloc for 4 dense matrix each decoder layer if SPARSITY_HAOJUN is detected,
+                               // unless SplitK==0 which means it is kept as dense during pruning
+#endif
+        deviceMalloc(&weights_ptr[2], hidden_units_ * 3 * hidden_units_ / tensor_para_size_);
     deviceMalloc(&weights_ptr[3], 3 * hidden_units_ / tensor_para_size_);
-    deviceMalloc(&weights_ptr[4], hidden_units_ / tensor_para_size_ * hidden_units_);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[1] == 0)  // Do not malloc for 4 dense matrix each decoder layer if SPARSITY_HAOJUN is detected,
+                               // unless SplitK==0 which means it is kept as dense during pruning
+#endif
+        deviceMalloc(&weights_ptr[4], hidden_units_ / tensor_para_size_ * hidden_units_);
     deviceMalloc(&weights_ptr[5], hidden_units_);
     deviceMalloc(&weights_ptr[6], hidden_units_);
     deviceMalloc(&weights_ptr[7], hidden_units_);
-
-    deviceMalloc(&weights_ptr[8], hidden_units_ * inter_size_ / tensor_para_size_);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[2] == 0)  // Do not malloc for 4 dense matrix each decoder layer if SPARSITY_HAOJUN is detected,
+                               // unless SplitK==0 which means it is kept as dense during pruning
+#endif
+        deviceMalloc(&weights_ptr[8], hidden_units_ * inter_size_ / tensor_para_size_);
     deviceMalloc(&weights_ptr[9], inter_size_ / tensor_para_size_);
-    deviceMalloc(&weights_ptr[10], inter_size_ / tensor_para_size_ * hidden_units_);
+#ifdef SPARSITY_HAOJUN
+    if (SplitK_List_[3] == 0)  // Do not malloc for 4 dense matrix each decoder layer if SPARSITY_HAOJUN is detected,
+                               // unless SplitK==0 which means it is kept as dense during pruning
+#endif
+        deviceMalloc(&weights_ptr[10], inter_size_ / tensor_para_size_ * hidden_units_);
     deviceMalloc(&weights_ptr[11], hidden_units_);
 
+#ifdef SPARSITY_HAOJUN
+    // Malloc memory for 4 sparse matrix, 8 memory space reserved for both NNZWeights and TileOffsets
+    // printf("Definition of SPARSITY_HAOJUN: %d\n", SPARSITY_HAOJUN);
+    FT_CHECK_WITH_INFO(NNZ_List_.size() == 4 && NumOffsets_List_.size() == 4,
+                       "ERROR: NNZ_List_ and NumOffsets_List_ should have 4 elements!\n");
+    for (int i = 0; i < 4; i++) {
+        if (SplitK_List_[i] != 0) {
+            check_cuda_error(cudaMalloc((void**)(&weights_ptr_NZ[i]), sizeof(unsigned int) * NNZ_List_[i]));
+            check_cuda_error(cudaMalloc((void**)(&weights_ptr_Offset[i]), sizeof(unsigned int) * NumOffsets_List_[i]));
+        }
+    }
+#endif
     if (gpt_variant_params_.has_adapters) {
         deviceMalloc(&weights_ptr[12], hidden_units_ * gpt_variant_params_.adapter_inter_size / tensor_para_size_);
         deviceMalloc(&weights_ptr[13], gpt_variant_params_.adapter_inter_size / tensor_para_size_);
@@ -460,18 +659,18 @@ template<typename T>
 void ParallelGptDecoderLayerWeight<T>::compress_weights(cublasMMWrapper& cublas_wrapper, int hidden_dim)
 {
     hidden_units_ = hidden_dim;
-    inter_size_   = 4 * hidden_units_;
-
-    const size_t num_sparse_weights            = 8;
-    size_t       shapes[num_sparse_weights][2] = {
-              {hidden_units_, 3 * hidden_units_ / tensor_para_size_},
-              {hidden_units_ / tensor_para_size_, hidden_units_},
-              {hidden_units_, inter_size_ / tensor_para_size_},
-              {inter_size_ / tensor_para_size_, hidden_units_},
-              {hidden_units_, gpt_variant_params_.adapter_inter_size / tensor_para_size_},
-              {gpt_variant_params_.adapter_inter_size / tensor_para_size_, hidden_units_},
-              {hidden_units_, gpt_variant_params_.adapter_inter_size / tensor_para_size_},
-              {gpt_variant_params_.adapter_inter_size / tensor_para_size_, hidden_units_}};
+    inter_size_ = 4 * hidden_units_;
+
+    const size_t num_sparse_weights = 8;
+    size_t shapes[num_sparse_weights][2] = {
+        {hidden_units_, 3 * hidden_units_ / tensor_para_size_},
+        {hidden_units_ / tensor_para_size_, hidden_units_},
+        {hidden_units_, inter_size_ / tensor_para_size_},
+        {inter_size_ / tensor_para_size_, hidden_units_},
+        {hidden_units_, gpt_variant_params_.adapter_inter_size / tensor_para_size_},
+        {gpt_variant_params_.adapter_inter_size / tensor_para_size_, hidden_units_},
+        {hidden_units_, gpt_variant_params_.adapter_inter_size / tensor_para_size_},
+        {gpt_variant_params_.adapter_inter_size / tensor_para_size_, hidden_units_}};
 
     const T* dense_weights[num_sparse_weights] = {self_attention_weights.query_weight.kernel,
                                                   self_attention_weights.attention_output_weight.kernel,
@@ -484,22 +683,22 @@ void ParallelGptDecoderLayerWeight<T>::compress_weights(cublasMMWrapper& cublas_
 
     size_t real_num_sparse_weights = gpt_variant_params_.has_adapters ? num_sparse_weights : (num_sparse_weights - 4);
     for (size_t i = 0; i < real_num_sparse_weights; ++i) {
-        int    m               = shapes[i][1];
-        int    k               = shapes[i][0];
+        int m = shapes[i][1];
+        int k = shapes[i][0];
         size_t compressed_size = cublas_wrapper.getSparseMatrixSize(m, k);
         deviceMalloc(&sp_weights_ptr[i], static_cast<int>(compressed_size), false);
         cublas_wrapper.compressMatrix(dense_weights[i], sp_weights_ptr[i], m, k);
     }
 
-    self_attention_weights.query_weight.sp_kernel                 = sp_weights_ptr[0];
-    self_attention_weights.attention_output_weight.sp_kernel      = sp_weights_ptr[1];
-    ffn_weights.intermediate_weight.sp_kernel                     = sp_weights_ptr[2];
-    ffn_weights.output_weight.sp_kernel                           = sp_weights_ptr[3];
+    self_attention_weights.query_weight.sp_kernel = sp_weights_ptr[0];
+    self_attention_weights.attention_output_weight.sp_kernel = sp_weights_ptr[1];
+    ffn_weights.intermediate_weight.sp_kernel = sp_weights_ptr[2];
+    ffn_weights.output_weight.sp_kernel = sp_weights_ptr[3];
     after_attention_adapter_weights.intermediate_weight.sp_kernel = sp_weights_ptr[4];
-    after_attention_adapter_weights.output_weight.sp_kernel       = sp_weights_ptr[5];
-    after_ffn_adapter_weights.intermediate_weight.sp_kernel       = sp_weights_ptr[6];
-    after_ffn_adapter_weights.output_weight.sp_kernel             = sp_weights_ptr[7];
-    is_maintain_sp_buffer                                         = true;
+    after_attention_adapter_weights.output_weight.sp_kernel = sp_weights_ptr[5];
+    after_ffn_adapter_weights.intermediate_weight.sp_kernel = sp_weights_ptr[6];
+    after_ffn_adapter_weights.output_weight.sp_kernel = sp_weights_ptr[7];
+    is_maintain_sp_buffer = true;
 }
 #endif
 
diff --git a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.h b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.h
index 2480ef6..1f1c680 100644
--- a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.h
+++ b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptDecoderLayerWeight.h
@@ -29,12 +29,12 @@ namespace fastertransformer {
 
 struct gptVariantParams {
     // GPT default params
-    float          layernorm_eps              = 1e-6f;
-    LayerNormType  layernorm_type             = LayerNormType::pre_layernorm;
-    ActivationType activation_type            = ActivationType::Gelu;
-    bool           has_post_decoder_layernorm = true;
+    float layernorm_eps = 1e-6f;
+    LayerNormType layernorm_type = LayerNormType::pre_layernorm;
+    ActivationType activation_type = ActivationType::Gelu;
+    bool has_post_decoder_layernorm = true;
     // detoxification adapters. refer to
-    bool   has_adapters       = false;
+    bool has_adapters = false;
     size_t adapter_inter_size = 0;
 };
 
@@ -43,16 +43,29 @@ struct ParallelGptDecoderLayerWeight {
 public:
     ParallelGptDecoderLayerWeight() = default;
     ParallelGptDecoderLayerWeight(const int int8_mode);
-    ParallelGptDecoderLayerWeight(const int        hidden_units,
-                                  const int        inter_size,
-                                  const int        tensor_para_size,
-                                  const int        tensor_para_rank,
-                                  const int        int8_mode          = 0,
+    ParallelGptDecoderLayerWeight(const int hidden_units,
+                                  const int inter_size,
+                                  const int tensor_para_size,
+                                  const int tensor_para_rank,
+                                  const int int8_mode = 0,
                                   gptVariantParams gpt_variant_params = {});
+
+#ifdef SPARSITY_HAOJUN
+    ParallelGptDecoderLayerWeight(const int hidden_units,
+                                  const int inter_size,
+                                  const int tensor_para_size,
+                                  const int tensor_para_rank,
+                                  const std::vector<int> NNZ_List,
+                                  const std::vector<int> NumOffsets_List,
+                                  const std::vector<int> SplitK_List,
+                                  const int int8_mode = 0,
+                                  gptVariantParams gpt_variant_params = {});
+#endif
+
     ~ParallelGptDecoderLayerWeight();
     ParallelGptDecoderLayerWeight(const ParallelGptDecoderLayerWeight& other);
     ParallelGptDecoderLayerWeight& operator=(const ParallelGptDecoderLayerWeight& other);
-    void                           loadModel(std::string dir_path, FtCudaDataType model_file_type);
+    void loadModel(std::string dir_path, FtCudaDataType model_file_type);
 #ifdef SPARSITY_ENABLED
     void compress_weights(cublasMMWrapper& cublas_wrapper, int hidden_dim);
 #endif
@@ -61,9 +74,9 @@ public:
     LayerNormWeight<T> pre_layernorm_weights;
     AttentionWeight<T> self_attention_weights;
     LayerNormWeight<T> self_attn_layernorm_weights;
-    FfnWeight<T>       ffn_weights;
-    FfnWeight<T>       after_attention_adapter_weights;
-    FfnWeight<T>       after_ffn_adapter_weights;
+    FfnWeight<T> ffn_weights;
+    FfnWeight<T> after_attention_adapter_weights;
+    FfnWeight<T> after_ffn_adapter_weights;
 
 private:
     void setWeightPtr();
@@ -72,24 +85,34 @@ private:
 protected:
     size_t hidden_units_;
     size_t inter_size_;
-    size_t tensor_para_size_  = 1;
-    size_t tensor_para_rank_  = 0;
-    bool   is_maintain_buffer = false;
-    int    int8_mode_         = 0;
+    size_t tensor_para_size_ = 1;
+    size_t tensor_para_rank_ = 0;
+    bool is_maintain_buffer = false;
+    int int8_mode_ = 0;
 
     // gpt varians params. e.g. detoxification adapters
     gptVariantParams gpt_variant_params_;
 
     std::vector<T*> weights_ptr = std::vector<T*>(20, nullptr);
 
+#ifdef SPARSITY_HAOJUN
+    const std::vector<int> NNZ_List_;
+    const std::vector<int> NumOffsets_List_;
+    const std::vector<int> SplitK_List_;
+    // 2 ptr for each sparse matrix, 4 sparse matrix for each decoder layer
+    // first 4 PTRs for NZWeight, next 4 PTRs for TileOffsets
+    std::vector<unsigned int*> weights_ptr_NZ = std::vector<unsigned int*>(4, nullptr);
+    std::vector<int*> weights_ptr_Offset = std::vector<int*>(4, nullptr);
+#endif
+
     std::vector<int8_t*> int8_weights_ptr = std::vector<int8_t*>(8, nullptr);
 
     std::vector<float*> scale_ptr = std::vector<float*>(8, nullptr);
-    cudaStream_t        stream_   = 0;
+    cudaStream_t stream_ = 0;
 
 #ifdef SPARSITY_ENABLED
-    std::vector<T*> sp_weights_ptr        = std::vector<T*>(8, nullptr);
-    bool            is_maintain_sp_buffer = false;
+    std::vector<T*> sp_weights_ptr = std::vector<T*>(8, nullptr);
+    bool is_maintain_sp_buffer = false;
 #endif
 };
 
diff --git a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.cc b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.cc
index 6d5a0c9..7da586e 100644
--- a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.cc
+++ b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.cc
@@ -18,20 +18,22 @@
 
 namespace fastertransformer {
 
+#ifdef SPARSITY_HAOJUN
 template<typename T>
-ParallelGptWeight<T>::ParallelGptWeight(const int                                  hidden_units,
-                                        const int                                  inter_size,
-                                        const int                                  vocab_size,
-                                        const int                                  num_layer,
-                                        const int                                  max_seq_len,
-                                        const int                                  tensor_para_size,
-                                        const int                                  tensor_para_rank,
-                                        const int                                  layer_para_size,
-                                        const int                                  layer_para_rank,
-                                        const int                                  int8_mode,
-                                        PromptLearningType                         prompt_learning_type,
+ParallelGptWeight<T>::ParallelGptWeight(const int hidden_units,
+                                        const int inter_size,
+                                        const int vocab_size,
+                                        const int num_layer,
+                                        const int max_seq_len,
+                                        const int tensor_para_size,
+                                        const int tensor_para_rank,
+                                        const int layer_para_size,
+                                        const int layer_para_rank,
+                                        const std::string dir_path,  //////
+                                        const int int8_mode,
+                                        PromptLearningType prompt_learning_type,
                                         std::map<std::string, std::pair<int, int>> prompt_learning_pair,
-                                        gptVariantParams                           gpt_variant_params):
+                                        gptVariantParams gpt_variant_params):
     hidden_units_(hidden_units),
     inter_size_(inter_size),
     vocab_size_(vocab_size),
@@ -59,6 +61,114 @@ ParallelGptWeight<T>::ParallelGptWeight(const int
     malloc_load_prompt_weights_ = !prompt_learning_pair_.empty()
                                   && (prompt_learning_type_ == PromptLearningType::p_prompt_tuning
                                       || prompt_learning_type_ == PromptLearningType::prefix_prompt);
+
+    // load the list of NNZ, NumOffsets, and SplitK for all the decoder layers
+    // Each file contains NumOfLayers * 4 int values
+    // self_attention_weights.query_weight -> self_attention_weights.attention_output_weight
+    //                                     -> ffn_weights.intermediate_weight
+    //                                     -> ffn_weights.output_weight
+    std::vector<int> NNZ_List_AllLayers = std::vector<int>(num_layer_ * 4);
+    std::vector<int> NumOffsets_List_AllLayers = std::vector<int>(num_layer_ * 4);
+    std::vector<int> SplitK_List_AllLayers = std::vector<int>(num_layer_ * 4);
+    loadDataArrayFromBin_To_CPU<int>(NNZ_List_AllLayers.data(),
+                                     num_layer_ * 4,
+                                     dir_path + "/NNZ_List." + std::to_string(tensor_para_rank_) + ".bin");
+    loadDataArrayFromBin_To_CPU<int>(NumOffsets_List_AllLayers.data(),
+                                     num_layer_ * 4,
+                                     dir_path + "/NumOffsets_List." + std::to_string(tensor_para_rank_) + ".bin");
+    loadDataArrayFromBin_To_CPU<int>(SplitK_List_AllLayers.data(),
+                                     num_layer_ * 4,
+                                     dir_path + "/SplitK_List." + std::to_string(tensor_para_rank_) + ".bin");
+    // printf("NNZ_List_AllLayers.size():%ld\n", NNZ_List_AllLayers.size());
+    FT_CHECK_WITH_INFO(NNZ_List_AllLayers.size() == num_layer_ * 4,
+                       "ERROR: NNZ_List_AllLayers           should have num_layer_*4 elements!\n");
+    FT_CHECK_WITH_INFO(NumOffsets_List_AllLayers.size() == num_layer_ * 4,
+                       "ERROR: NumOffsets_List_AllLayers    should have num_layer_*4 elements!\n");
+    FT_CHECK_WITH_INFO(SplitK_List_AllLayers.size() == num_layer_ * 4,
+                       "ERROR: SplitK_List_AllLayers        should have num_layer_*4 elements!\n");
+    //
+    decoder_layer_weights.clear();
+    decoder_layer_weights.reserve(num_layer_);
+    for (int l = 0; l < num_layer_; l++) {
+        if (isValidLayerParallelId(l)) {
+            const std::vector<int> NNZ_List{NNZ_List_AllLayers[l * 4],
+                                            NNZ_List_AllLayers[l * 4 + 1],
+                                            NNZ_List_AllLayers[l * 4 + 2],
+                                            NNZ_List_AllLayers[l * 4 + 3]};
+            // printf("NNZ_List: %d %d %d %d\n", NNZ_List_AllLayers[l*4],           NNZ_List_AllLayers[l*4+1],
+            // NNZ_List_AllLayers[l*4+2],          NNZ_List_AllLayers[l*4+3]);
+            const std::vector<int> NumOffsets_List{NumOffsets_List_AllLayers[l * 4],
+                                                   NumOffsets_List_AllLayers[l * 4 + 1],
+                                                   NumOffsets_List_AllLayers[l * 4 + 2],
+                                                   NumOffsets_List_AllLayers[l * 4 + 3]};
+            const std::vector<int> SplitK_List{SplitK_List_AllLayers[l * 4],
+                                               SplitK_List_AllLayers[l * 4 + 1],
+                                               SplitK_List_AllLayers[l * 4 + 2],
+                                               SplitK_List_AllLayers[l * 4 + 3]};
+            decoder_layer_weights.push_back(new ParallelGptDecoderLayerWeight<T>(hidden_units_,
+                                                                                 inter_size_,
+                                                                                 tensor_para_size_,
+                                                                                 tensor_para_rank_,
+                                                                                 NNZ_List,
+                                                                                 NumOffsets_List,
+                                                                                 SplitK_List,
+                                                                                 int8_mode_,
+                                                                                 gpt_variant_params_));
+        }
+        else {
+            // Don't malloc and load these layers since we don't use them.
+            decoder_layer_weights.push_back(new ParallelGptDecoderLayerWeight<T>());
+        }
+    }
+
+    mallocWeights();
+    setWeightPtr();
+}
+#endif
+
+template<typename T>
+ParallelGptWeight<T>::ParallelGptWeight(const int hidden_units,
+                                        const int inter_size,
+                                        const int vocab_size,
+                                        const int num_layer,
+                                        const int max_seq_len,
+                                        const int tensor_para_size,
+                                        const int tensor_para_rank,
+                                        const int layer_para_size,
+                                        const int layer_para_rank,
+                                        const int int8_mode,
+                                        PromptLearningType prompt_learning_type,
+                                        std::map<std::string, std::pair<int, int>> prompt_learning_pair,
+                                        gptVariantParams gpt_variant_params):
+    hidden_units_(hidden_units),
+    inter_size_(inter_size),
+    vocab_size_(vocab_size),
+    num_layer_(num_layer),
+    max_seq_len_(max_seq_len),
+    tensor_para_size_(tensor_para_size),
+    tensor_para_rank_(tensor_para_rank),
+    layer_para_size_(layer_para_size),
+    layer_para_rank_(layer_para_rank),
+    int8_mode_(int8_mode),
+    prompt_learning_type_(prompt_learning_type),
+    prompt_learning_pair_(prompt_learning_pair),
+    gpt_variant_params_(gpt_variant_params)
+{
+    FT_CHECK(num_layer_ % layer_para_size_ == 0);
+    // set prompt weight size
+    if (prompt_learning_type_ == PromptLearningType::prefix_prompt) {
+        prompt_token_weight_size_ = 2 * num_layer_ * hidden_units_ / tensor_para_size_;
+    }
+    else if (prompt_learning_type_ == PromptLearningType::p_prompt_tuning) {
+        prompt_token_weight_size_ = hidden_units_;
+    }
+
+    // set if load and malloc prompt weights
+    malloc_load_prompt_weights_ = !prompt_learning_pair_.empty()
+                                  && (prompt_learning_type_ == PromptLearningType::p_prompt_tuning
+                                      || prompt_learning_type_ == PromptLearningType::prefix_prompt);
+
+    //
     decoder_layer_weights.clear();
     decoder_layer_weights.reserve(num_layer_);
     for (int l = 0; l < num_layer_; l++) {
@@ -84,13 +194,13 @@ ParallelGptWeight<T>::~ParallelGptWeight()
             deviceFree(weights_ptr[i]);
         }
 
-        position_encoding_table       = nullptr;
-        pre_decoder_embedding_table   = nullptr;
-        post_decoder_layernorm.beta   = nullptr;
-        post_decoder_layernorm.gamma  = nullptr;
+        position_encoding_table = nullptr;
+        pre_decoder_embedding_table = nullptr;
+        post_decoder_layernorm.beta = nullptr;
+        post_decoder_layernorm.gamma = nullptr;
         post_decoder_embedding.kernel = nullptr;
-        post_decoder_embedding.bias   = nullptr;
-        is_maintain_buffer            = false;
+        post_decoder_embedding.bias = nullptr;
+        is_maintain_buffer = false;
     }
 
     for (int i = 0; i < num_layer_; i++) {
@@ -116,6 +226,12 @@ ParallelGptWeight<T>::ParallelGptWeight(const ParallelGptWeight& other):
     prompt_learning_pair_(other.prompt_learning_pair_),
     gpt_variant_params_(other.gpt_variant_params_)
 {
+#ifdef SPARSITY_HAOJUN
+    printf("The copy construction function for ParallelGptWeight<T> is called.\n ");
+    printf("However, this function is not implemented yet!\n");
+    exit(-1);
+#endif
+
     mallocWeights();
     cudaD2Dcpy(weights_ptr[0], other.weights_ptr[0], max_seq_len_ * vocab_size_);
     cudaD2Dcpy(weights_ptr[1], other.weights_ptr[1], vocab_size_ * hidden_units_);
@@ -126,10 +242,10 @@ ParallelGptWeight<T>::ParallelGptWeight(const ParallelGptWeight& other):
     // prompt learning table: malloc weights and set weight ptr
     if (malloc_load_prompt_weights_) {
         for (auto const& prompt : prompt_learning_pair_) {
-            std::string task_name     = prompt.first;
-            int         task_name_id  = prompt.second.first;
-            int         prompt_length = prompt.second.second;
-            size_t      prompt_id     = num_base_weights + (size_t)task_name_id;
+            std::string task_name = prompt.first;
+            int task_name_id = prompt.second.first;
+            int prompt_length = prompt.second.second;
+            size_t prompt_id = num_base_weights + (size_t)task_name_id;
 
             // cuda device to device memcpy prompt table weights buffer memory
             cudaD2Dcpy(weights_ptr[prompt_id], other.weights_ptr[prompt_id], prompt_length * prompt_token_weight_size_);
@@ -148,21 +264,26 @@ ParallelGptWeight<T>::ParallelGptWeight(const ParallelGptWeight& other):
 template<typename T>
 ParallelGptWeight<T>& ParallelGptWeight<T>::operator=(const ParallelGptWeight& other)
 {
-    hidden_units_               = other.hidden_units_;
-    inter_size_                 = other.inter_size_;
-    num_layer_                  = other.num_layer_;
-    vocab_size_                 = other.vocab_size_;
-    max_seq_len_                = other.max_seq_len_;
-    tensor_para_size_           = other.tensor_para_size_;
-    tensor_para_rank_           = other.tensor_para_rank_;
-    layer_para_size_            = other.layer_para_size_;
-    layer_para_rank_            = other.layer_para_rank_;
-    int8_mode_                  = other.int8_mode_;
-    prompt_token_weight_size_   = other.prompt_token_weight_size_;
+#ifdef SPARSITY_HAOJUN
+    printf("The copy construction function (operator=) for ParallelGptWeight<T> is called.\n ");
+    printf("However, this function is not implemented yet!\n");
+    exit(-1);
+#endif
+    hidden_units_ = other.hidden_units_;
+    inter_size_ = other.inter_size_;
+    num_layer_ = other.num_layer_;
+    vocab_size_ = other.vocab_size_;
+    max_seq_len_ = other.max_seq_len_;
+    tensor_para_size_ = other.tensor_para_size_;
+    tensor_para_rank_ = other.tensor_para_rank_;
+    layer_para_size_ = other.layer_para_size_;
+    layer_para_rank_ = other.layer_para_rank_;
+    int8_mode_ = other.int8_mode_;
+    prompt_token_weight_size_ = other.prompt_token_weight_size_;
     malloc_load_prompt_weights_ = other.malloc_load_prompt_weights_;
-    prompt_learning_type_       = other.prompt_learning_type_;
-    prompt_learning_pair_       = other.prompt_learning_pair_;
-    gpt_variant_params_         = other.gpt_variant_params_;
+    prompt_learning_type_ = other.prompt_learning_type_;
+    prompt_learning_pair_ = other.prompt_learning_pair_;
+    gpt_variant_params_ = other.gpt_variant_params_;
 
     mallocWeights();
     cudaD2Dcpy(weights_ptr[0], other.weights_ptr[0], max_seq_len_ * vocab_size_);
@@ -174,9 +295,9 @@ ParallelGptWeight<T>& ParallelGptWeight<T>::operator=(const ParallelGptWeight& o
     // prompt learning tables: malloc weights and set weight ptr
     if (malloc_load_prompt_weights_) {
         for (auto const& prompt : prompt_learning_pair_) {
-            int    task_name_id  = prompt.second.first;
-            int    prompt_length = prompt.second.second;
-            size_t prompt_id     = num_base_weights + (size_t)task_name_id;
+            int task_name_id = prompt.second.first;
+            int prompt_length = prompt.second.second;
+            size_t prompt_id = num_base_weights + (size_t)task_name_id;
 
             // cuda device to device memcpy prompt weights buffer memory
             cudaD2Dcpy(weights_ptr[prompt_id], other.weights_ptr[prompt_id], prompt_length * prompt_token_weight_size_);
@@ -198,18 +319,18 @@ void ParallelGptWeight<T>::setWeightPtr()
 {
     prompt_learning_table.resize(prompt_learning_pair_.size());
 
-    position_encoding_table       = weights_ptr[0];
-    pre_decoder_embedding_table   = weights_ptr[1];
-    post_decoder_layernorm.beta   = weights_ptr[2];
-    post_decoder_layernorm.gamma  = weights_ptr[3];
+    position_encoding_table = weights_ptr[0];
+    pre_decoder_embedding_table = weights_ptr[1];
+    post_decoder_layernorm.beta = weights_ptr[2];
+    post_decoder_layernorm.gamma = weights_ptr[3];
     post_decoder_embedding.kernel = weights_ptr[4];
-    post_decoder_embedding.bias   = nullptr;
+    post_decoder_embedding.bias = nullptr;
 
     // prompt learning tables: set weight ptr
     if (malloc_load_prompt_weights_) {
         for (auto const& prompt : prompt_learning_pair_) {
-            int    task_name_id   = prompt.second.first;
-            int    prompt_length  = prompt.second.second;
+            int task_name_id = prompt.second.first;
+            int prompt_length = prompt.second.second;
             size_t task_weight_id = num_base_weights + (size_t)task_name_id;
 
             // set weight ptr
@@ -232,8 +353,8 @@ void ParallelGptWeight<T>::mallocWeights()
     // prompt learning tables: malloc weights
     if (malloc_load_prompt_weights_) {
         for (auto const& prompt : prompt_learning_pair_) {
-            int    task_name_id   = prompt.second.first;
-            int    prompt_length  = prompt.second.second;
+            int task_name_id = prompt.second.first;
+            int prompt_length = prompt.second.second;
             size_t task_weight_id = num_base_weights + (size_t)task_name_id;
 
             // malloc weights
@@ -270,10 +391,10 @@ void ParallelGptWeight<T>::loadModel(std::string dir_path)
     // prompt table: load weights from bin
     if (malloc_load_prompt_weights_) {
         for (auto const& prompt : prompt_learning_pair_) {
-            std::string task_name      = prompt.first;
-            int         task_name_id   = prompt.second.first;
-            int         prompt_length  = prompt.second.second;
-            size_t      task_weight_id = num_base_weights + (size_t)task_name_id;
+            std::string task_name = prompt.first;
+            int task_name_id = prompt.second.first;
+            int prompt_length = prompt.second.second;
+            size_t task_weight_id = num_base_weights + (size_t)task_name_id;
 
             std::string prompt_weight_path_name = (prompt_learning_type_ == PromptLearningType::p_prompt_tuning) ?
                                                       (dir_path + "/model.prompt_table." + task_name + ".weight.bin") :
diff --git a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.h b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.h
index 38458b1..b6d0585 100644
--- a/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.h
+++ b/src/fastertransformer/models/multi_gpu_gpt/ParallelGptWeight.h
@@ -29,33 +29,51 @@ template<typename T>
 struct ParallelGptWeight {
 
     ParallelGptWeight() = default;
-    ParallelGptWeight(const int                                  hidden_units,
-                      const int                                  inter_size,
-                      const int                                  vocab_size,
-                      const int                                  num_layer,
-                      const int                                  max_seq_len,
-                      const int                                  tensor_para_size,
-                      const int                                  tensor_para_rank,
-                      const int                                  layer_para_size,
-                      const int                                  layer_para_rank,
-                      const int                                  int8_mode            = 0,
-                      PromptLearningType                         prompt_learning_type = PromptLearningType::no_prompt,
+
+#ifdef SPARSITY_HAOJUN
+    ParallelGptWeight(const int hidden_units,
+                      const int inter_size,
+                      const int vocab_size,
+                      const int num_layer,
+                      const int max_seq_len,
+                      const int tensor_para_size,
+                      const int tensor_para_rank,
+                      const int layer_para_size,
+                      const int layer_para_rank,
+                      const std::string dir_path,
+                      const int int8_mode = 0,
+                      PromptLearningType prompt_learning_type = PromptLearningType::no_prompt,
+                      std::map<std::string, std::pair<int, int>> prompt_learning_pair = {},
+                      gptVariantParams gpt_variant_params = {});
+#endif
+
+    ParallelGptWeight(const int hidden_units,
+                      const int inter_size,
+                      const int vocab_size,
+                      const int num_layer,
+                      const int max_seq_len,
+                      const int tensor_para_size,
+                      const int tensor_para_rank,
+                      const int layer_para_size,
+                      const int layer_para_rank,
+                      const int int8_mode = 0,
+                      PromptLearningType prompt_learning_type = PromptLearningType::no_prompt,
                       std::map<std::string, std::pair<int, int>> prompt_learning_pair = {},
-                      gptVariantParams                           gpt_variant_params   = {});
+                      gptVariantParams gpt_variant_params = {});
     ~ParallelGptWeight();
     ParallelGptWeight(const ParallelGptWeight& other);
     ParallelGptWeight& operator=(const ParallelGptWeight& other);
-    void               loadModel(std::string dir_path);
-    void               resizeLayer(const int num_layer, const int int8_mode = 0);
+    void loadModel(std::string dir_path);
+    void resizeLayer(const int num_layer, const int int8_mode = 0);
 #ifdef SPARSITY_ENABLED
     void compress_weights(cublasMMWrapper& cublas_wrapper);
 #endif
 
     std::vector<ParallelGptDecoderLayerWeight<T>*> decoder_layer_weights;
-    const T*                                       position_encoding_table     = nullptr;
-    const T*                                       pre_decoder_embedding_table = nullptr;
-    LayerNormWeight<T>                             post_decoder_layernorm;
-    DenseWeight<T>                                 post_decoder_embedding;
+    const T* position_encoding_table = nullptr;
+    const T* pre_decoder_embedding_table = nullptr;
+    LayerNormWeight<T> post_decoder_layernorm;
+    DenseWeight<T> post_decoder_embedding;
 
     /*
        prompt_learning_pair = vectors of [weight ptr, prompt length] pair
@@ -65,7 +83,7 @@ struct ParallelGptWeight {
        idx is the task_name_id of the prompt tables
     */
     std::vector<std::pair<const T*, int>> prompt_learning_table = {};
-    inline size_t                         getMaxSeqLen() const
+    inline size_t getMaxSeqLen() const
     {
         return max_seq_len_;
     }
@@ -94,15 +112,15 @@ private:
     gptVariantParams gpt_variant_params_;
 
     // prompt learning pair (task_name, (task_name_id, prompt_len))
-    PromptLearningType                         prompt_learning_type_;
+    PromptLearningType prompt_learning_type_;
     std::map<std::string, std::pair<int, int>> prompt_learning_pair_;
-    bool                                       malloc_load_prompt_weights_ = false;
+    bool malloc_load_prompt_weights_ = false;
     // each prompt token's weight size
     size_t prompt_token_weight_size_ = 0;
 
-    bool            is_maintain_buffer = false;
-    size_t          num_base_weights   = 5;
-    std::vector<T*> weights_ptr        = std::vector<T*>(num_base_weights);
+    bool is_maintain_buffer = false;
+    size_t num_base_weights = 5;
+    std::vector<T*> weights_ptr = std::vector<T*>(num_base_weights);
 };
 
 }  // namespace fastertransformer
diff --git a/src/fastertransformer/utils/CMakeLists.txt b/src/fastertransformer/utils/CMakeLists.txt
index 9e84306..11fc6d1 100644
--- a/src/fastertransformer/utils/CMakeLists.txt
+++ b/src/fastertransformer/utils/CMakeLists.txt
@@ -26,6 +26,10 @@ set_property(TARGET cublasMMWrapper PROPERTY POSITION_INDEPENDENT_CODE  ON)
 set_property(TARGET cublasMMWrapper PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS  ON)
 if (SPARSITY_SUPPORT)
 target_link_libraries(cublasMMWrapper PUBLIC -lcublas -lcudart -lcurand -lcusparse -lcusparseLt cublasAlgoMap)
+# Haojun
+elseif (FLASH_LLM)
+target_link_libraries(cublasMMWrapper PUBLIC -lcublas -lcudart -lcurand cublasAlgoMap -lSpMM_API)
+#
 else()
 target_link_libraries(cublasMMWrapper PUBLIC -lcublas -lcudart -lcurand cublasAlgoMap)
 endif()
diff --git a/src/fastertransformer/utils/cublasMMWrapper.cc b/src/fastertransformer/utils/cublasMMWrapper.cc
index ea9c10a..e609af9 100644
--- a/src/fastertransformer/utils/cublasMMWrapper.cc
+++ b/src/fastertransformer/utils/cublasMMWrapper.cc
@@ -21,12 +21,12 @@
 #endif
 
 namespace fastertransformer {
-cublasMMWrapper::cublasMMWrapper(cublasHandle_t   cublas_handle,
+cublasMMWrapper::cublasMMWrapper(cublasHandle_t cublas_handle,
                                  cublasLtHandle_t cublaslt_handle,
-                                 cudaStream_t     stream,
-                                 cublasAlgoMap*   cublas_algo_map,
-                                 std::mutex*      mu,
-                                 IAllocator*      allocator):
+                                 cudaStream_t stream,
+                                 cublasAlgoMap* cublas_algo_map,
+                                 std::mutex* mu,
+                                 IAllocator* allocator):
     cublas_handle_(cublas_handle),
     cublaslt_handle_(cublaslt_handle),
     stream_(stream),
@@ -38,16 +38,31 @@ cublasMMWrapper::cublasMMWrapper(cublasHandle_t   cublas_handle,
     if (allocator_ != nullptr) {
         cublas_workspace_ = allocator_->reMalloc(cublas_workspace_, CUBLAS_WORKSPACE_SIZE);
     }
+#ifdef SPARSITY_HAOJUN
+    // For Decoder: Max_SplitK=32, Max_HiddenSize=16K, Max_BatchSize=128.
+    // For Context Decoder, Max_SplitK=1, ReductionSpace is not used.
+    Bytes_SpMM_ReductionSpace = 32 * 16 * 1024 * 128 * sizeof(half);
+    check_cuda_error(cudaMalloc(&SpMM_ReductionSpace, Bytes_SpMM_ReductionSpace));
+    if (!SpMM_ReductionSpace) {
+        printf("Errors in cublasMMWrapper.cc!\n");
+        printf("Falied in malloc GPU memory for SpMM Reduction.\n");
+        exit(-1);
+    }
+#ifdef PRINT_DEBUG_INFO
+    printf("Workspace for SpMM reduction is allocated in GPU memory: %d MByte.\n",
+           int(Bytes_SpMM_ReductionSpace / 1000000.0f));
+#endif
+#endif
 }
 
 #ifdef SPARSITY_ENABLED
-cublasMMWrapper::cublasMMWrapper(cublasHandle_t     cublas_handle,
-                                 cublasLtHandle_t   cublaslt_handle,
+cublasMMWrapper::cublasMMWrapper(cublasHandle_t cublas_handle,
+                                 cublasLtHandle_t cublaslt_handle,
                                  cusparseLtHandle_t cusparselt_handle,
-                                 cudaStream_t       stream,
-                                 cublasAlgoMap*     cublas_algo_map,
-                                 std::mutex*        mu,
-                                 IAllocator*        allocator):
+                                 cudaStream_t stream,
+                                 cublasAlgoMap* cublas_algo_map,
+                                 std::mutex* mu,
+                                 IAllocator* allocator):
     cublas_handle_(cublas_handle),
     cublaslt_handle_(cublaslt_handle),
     cusparselt_handle_(cusparselt_handle),
@@ -70,6 +85,16 @@ cublasMMWrapper::~cublasMMWrapper()
     if (allocator_ != nullptr) {
         allocator_->free((void**)(&cublas_workspace_));
     }
+#ifdef SPARSITY_HAOJUN
+    // For Decoder: Max_SplitK=32, Max_HiddenSize=16K, Max_BatchSize=128.
+    // For Context Decoder, Max_SplitK=1, ReductionSpace is not used.
+    check_cuda_error(cudaFree(SpMM_ReductionSpace));
+    SpMM_ReductionSpace = nullptr;
+    Bytes_SpMM_ReductionSpace = 0;
+#ifdef PRINT_DEBUG_INFO
+    printf("Workspace for SpMM reduction is released.\n");
+#endif
+#endif
 }
 
 cublasMMWrapper::cublasMMWrapper(const cublasMMWrapper& wrapper):
@@ -83,26 +108,57 @@ cublasMMWrapper::cublasMMWrapper(const cublasMMWrapper& wrapper):
     mu_(wrapper.mu_),
     allocator_(wrapper.allocator_)
 {
+#ifdef SPARSITY_HAOJUN
+    SpMM_ReductionSpace = wrapper.SpMM_ReductionSpace;
+    Bytes_SpMM_ReductionSpace = wrapper.Bytes_SpMM_ReductionSpace;
+#endif
+}
+
+#ifdef SPARSITY_HAOJUN
+void cublasMMWrapper::SpMM(const unsigned int* Compressed_A,
+                           const int* TileOffsets,
+                           const void* B,
+                           void* C,
+                           const int M_Global,
+                           const int N_Global,
+                           const int K_Global,
+                           int Split_K)
+{
+    mu_->lock();
+    SpMM_SplitK_API(stream_,
+                    static_cast<half*>(nullptr),
+                    reinterpret_cast<const uint4*>(Compressed_A),
+                    TileOffsets,
+                    reinterpret_cast<const half*>(B),
+                    reinterpret_cast<half*>(C),
+                    M_Global,
+                    N_Global,
+                    K_Global,
+                    SpMM_ReductionSpace,
+                    Split_K);
+    sync_check_cuda_error();
+    mu_->unlock();
 }
+#endif
 
 void cublasMMWrapper::Gemm(cublasOperation_t transa,
                            cublasOperation_t transb,
-                           const int         m,
-                           const int         n,
-                           const int         k,
-                           const void*       alpha,
-                           const void*       A,
-                           cudaDataType_t    Atype,
-                           int               lda,
-                           const void*       B,
-                           cudaDataType_t    Btype,
-                           int               ldb,
-                           const void*       beta,
-                           void*             C,
-                           cudaDataType_t    Ctype,
-                           int               ldc,
-                           cudaDataType_t    computeType,
-                           cublasGemmAlgo_t  algo)
+                           const int m,
+                           const int n,
+                           const int k,
+                           const void* alpha,
+                           const void* A,
+                           cudaDataType_t Atype,
+                           int lda,
+                           const void* B,
+                           cudaDataType_t Btype,
+                           int ldb,
+                           const void* beta,
+                           void* C,
+                           cudaDataType_t Ctype,
+                           int ldc,
+                           cudaDataType_t computeType,
+                           cublasGemmAlgo_t algo)
 {
     mu_->lock();
     check_cuda_error(cublasGemmEx(cublas_handle_,
@@ -130,45 +186,45 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
 
 void cublasMMWrapper::Gemm(cublasOperation_t transa,
                            cublasOperation_t transb,
-                           const int         m,
-                           const int         n,
-                           const int         k,
-                           const void*       A,
-                           const int         lda,
-                           const void*       B,
-                           const int         ldb,
-                           void*             C,
-                           const int         ldc)
+                           const int m,
+                           const int n,
+                           const int k,
+                           const void* A,
+                           const int lda,
+                           const void* B,
+                           const int ldb,
+                           void* C,
+                           const int ldc)
 {
     Gemm(transa, transb, m, n, k, A, lda, B, ldb, C, ldc, 1.0f, 0.0f);
 }
 
 void cublasMMWrapper::Gemm(cublasOperation_t transa,
                            cublasOperation_t transb,
-                           const int         m,
-                           const int         n,
-                           const int         k,
-                           const void*       A,
-                           const int         lda,
-                           const void*       B,
-                           const int         ldb,
-                           void*             C,
-                           const int         ldc,
-                           float             f_alpha,
-                           float             f_beta)
+                           const int m,
+                           const int n,
+                           const int k,
+                           const void* A,
+                           const int lda,
+                           const void* B,
+                           const int ldb,
+                           void* C,
+                           const int ldc,
+                           float f_alpha,
+                           float f_beta)
 {
     half h_alpha = (half)(f_alpha);
-    half h_beta  = (half)(f_beta);
+    half h_beta = (half)(f_beta);
 
     mu_->lock();
     // TODO: default cublas libs
-    int  is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
-    bool using_cublasLt      = (Atype_ == CUDA_R_16F) ? true : false;
-    int  batch_count         = 1;
+    int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
+    bool using_cublasLt = (Atype_ == CUDA_R_16F) ? true : false;
+    int batch_count = 1;
     // fp32 use cublas as default
     // fp16 use cublasLt as default
     const void* alpha = is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<void*>(&f_alpha);
-    const void* beta  = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<void*>(&f_beta);
+    const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<void*>(&f_beta);
 
     int findAlgo = cublas_algo_map_->isExist(batch_count, m, n, k, getCublasDataType(Atype_));
 
@@ -183,9 +239,9 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
     }
 
     if (using_cublasLt) {
-        cublasLtMatmulDesc_t   operationDesc = NULL;
+        cublasLtMatmulDesc_t operationDesc = NULL;
         cublasLtMatrixLayout_t Adesc = NULL, Bdesc = NULL, Cdesc = NULL;
-        cudaDataType_t         scaleType;
+        cudaDataType_t scaleType;
 #if (CUDART_VERSION >= 11000)
         cublasComputeType_t computeType;
 #else
@@ -224,8 +280,8 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
         cublasLtMatmulDescSetAttribute(operationDesc, CUBLASLT_MATMUL_DESC_TRANSB, &transb, sizeof(cublasOperation_t));
 
         cublasLtMatmulAlgo_t algo;
-        void*                workSpace     = cublas_workspace_;
-        int                  workspaceSize = cublas_workspace_ == NULL ? 0 : CUBLAS_WORKSPACE_SIZE;
+        void* workSpace = cublas_workspace_;
+        int workspaceSize = cublas_workspace_ == NULL ? 0 : CUBLAS_WORKSPACE_SIZE;
         if (findAlgo) {
             if (info.workspaceSize > workspaceSize) {
                 findAlgo = 0;
@@ -301,26 +357,26 @@ void cublasMMWrapper::Gemm(cublasOperation_t transa,
 
 void cublasMMWrapper::setFP32GemmConfig()
 {
-    Atype_       = CUDA_R_32F;
-    Btype_       = CUDA_R_32F;
-    Ctype_       = CUDA_R_32F;
+    Atype_ = CUDA_R_32F;
+    Btype_ = CUDA_R_32F;
+    Ctype_ = CUDA_R_32F;
     computeType_ = CUDA_R_32F;
 }
 
 void cublasMMWrapper::setFP16GemmConfig()
 {
-    Atype_       = CUDA_R_16F;
-    Btype_       = CUDA_R_16F;
-    Ctype_       = CUDA_R_16F;
+    Atype_ = CUDA_R_16F;
+    Btype_ = CUDA_R_16F;
+    Ctype_ = CUDA_R_16F;
     computeType_ = CUDA_R_32F;
 }
 
 #ifdef ENABLE_BF16
 void cublasMMWrapper::setBF16GemmConfig()
 {
-    Atype_       = CUDA_R_16BF;
-    Btype_       = CUDA_R_16BF;
-    Ctype_       = CUDA_R_16BF;
+    Atype_ = CUDA_R_16BF;
+    Btype_ = CUDA_R_16BF;
+    Ctype_ = CUDA_R_16BF;
     computeType_ = CUDA_R_32F;
 }
 #endif
@@ -330,9 +386,9 @@ void cublasMMWrapper::setGemmConfig(cudaDataType_t aType,
                                     cudaDataType_t cType,
                                     cudaDataType_t computeType)
 {
-    Atype_       = aType;
-    Btype_       = bType;
-    Ctype_       = cType;
+    Atype_ = aType;
+    Btype_ = bType;
+    Ctype_ = cType;
     computeType_ = computeType;
 }
 
@@ -357,58 +413,58 @@ CublasDataType cublasMMWrapper::getCublasDataType(cudaDataType_t data_type)
 // only works for cublas 11.x
 void cublasMMWrapper::Gemm(cublasOperation_t transa,
                            cublasOperation_t transb,
-                           const int         m,
-                           const int         n,
-                           const int         k,
-                           const void*       A,
-                           const int         lda,
-                           const void*       B,
-                           const int         ldb,
-                           const void*       bias,
-                           void*             C,
-                           const int         ldc)
+                           const int m,
+                           const int n,
+                           const int k,
+                           const void* A,
+                           const int lda,
+                           const void* B,
+                           const int ldb,
+                           const void* bias,
+                           void* C,
+                           const int ldc)
 {
-    cudaDataType_t      Atype, Btype, Ctype;
+    cudaDataType_t Atype, Btype, Ctype;
     cublasComputeType_t computeType;
-    cudaDataType_t      scaleType;
-    float               alpha_float = 1.0f;
-    float               beta_float  = 0.0f;
-    half                alpha_half  = half(1.0f);
-    half                beta_half   = half(0.0f);
-    void *              alpha, *beta;
+    cudaDataType_t scaleType;
+    float alpha_float = 1.0f;
+    float beta_float = 0.0f;
+    half alpha_half = half(1.0f);
+    half beta_half = half(0.0f);
+    void *alpha, *beta;
 
     // int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
     if (Atype_ == CUDA_R_32F) {
         computeType = CUBLAS_COMPUTE_32F_FAST_TF32;
-        Atype       = CUDA_R_32F;
-        Btype       = CUDA_R_32F;
-        Ctype       = CUDA_R_32F;
-        scaleType   = CUDA_R_32F;
-        alpha       = &alpha_float;
-        beta        = &beta_float;
+        Atype = CUDA_R_32F;
+        Btype = CUDA_R_32F;
+        Ctype = CUDA_R_32F;
+        scaleType = CUDA_R_32F;
+        alpha = &alpha_float;
+        beta = &beta_float;
     }
     else if (Atype_ == CUDA_R_16BF) {
         computeType = CUBLAS_COMPUTE_32F_FAST_TF32;
-        Atype       = CUDA_R_16BF;
-        Btype       = CUDA_R_16BF;
-        Ctype       = CUDA_R_16BF;
-        scaleType   = CUDA_R_32F;
-        alpha       = &alpha_float;
-        beta        = &beta_float;
+        Atype = CUDA_R_16BF;
+        Btype = CUDA_R_16BF;
+        Ctype = CUDA_R_16BF;
+        scaleType = CUDA_R_32F;
+        alpha = &alpha_float;
+        beta = &beta_float;
     }
     else {
         computeType = CUBLAS_COMPUTE_16F;
-        Atype       = CUDA_R_16F;
-        Btype       = CUDA_R_16F;
-        Ctype       = CUDA_R_16F;
-        scaleType   = CUDA_R_16F;
-        alpha       = &alpha_half;
-        beta        = &beta_half;
+        Atype = CUDA_R_16F;
+        Btype = CUDA_R_16F;
+        Ctype = CUDA_R_16F;
+        scaleType = CUDA_R_16F;
+        alpha = &alpha_half;
+        beta = &beta_half;
     }
 
-    cublasLtMatmulDesc_t   operationDesc = NULL;
+    cublasLtMatmulDesc_t operationDesc = NULL;
     cublasLtMatrixLayout_t Adesc = NULL, Bdesc = NULL, Cdesc = NULL;
-    cublasLtEpilogue_t     epi = CUBLASLT_EPILOGUE_BIAS;
+    cublasLtEpilogue_t epi = CUBLASLT_EPILOGUE_BIAS;
     cublasLtMatrixLayoutCreate(&Adesc, Atype, (transa == CUBLAS_OP_N) ? m : k, (transa == CUBLAS_OP_N) ? k : m, lda);
     cublasLtMatrixLayoutCreate(&Bdesc, Btype, (transb == CUBLAS_OP_N) ? k : n, (transb == CUBLAS_OP_N) ? n : k, ldb);
     cublasLtMatrixLayoutCreate(&Cdesc, Ctype, m, n, ldc);
@@ -433,27 +489,27 @@ void cublasMMWrapper::setStream(cudaStream_t stream)
 
 void cublasMMWrapper::stridedBatchedGemm(cublasOperation_t transa,
                                          cublasOperation_t transb,
-                                         const int         m,
-                                         const int         n,
-                                         const int         k,
-                                         const void*       A,
-                                         const int         lda,
-                                         const int64_t     strideA,
-                                         const void*       B,
-                                         const int         ldb,
-                                         const int64_t     strideB,
-                                         void*             C,
-                                         const int         ldc,
-                                         const int64_t     strideC,
-                                         const int         batch_count,
-                                         const float       f_alpha,
-                                         const float       f_beta)
+                                         const int m,
+                                         const int n,
+                                         const int k,
+                                         const void* A,
+                                         const int lda,
+                                         const int64_t strideA,
+                                         const void* B,
+                                         const int ldb,
+                                         const int64_t strideB,
+                                         void* C,
+                                         const int ldc,
+                                         const int64_t strideC,
+                                         const int batch_count,
+                                         const float f_alpha,
+                                         const float f_beta)
 {
     half h_alpha = (half)f_alpha;
-    half h_beta  = (half)f_beta;
+    half h_beta = (half)f_beta;
 
     mu_->lock();
-    int         is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
     const void* alpha =
         is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<const void*>(&f_alpha);
     const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<const void*>(&f_beta);
@@ -488,31 +544,31 @@ void cublasMMWrapper::stridedBatchedGemm(cublasOperation_t transa,
 
 void cublasMMWrapper::stridedBatchedGemm(cublasOperation_t transa,
                                          cublasOperation_t transb,
-                                         const int         m,
-                                         const int         n,
-                                         const int         k,
-                                         const float       f_alpha,
-                                         const void*       A,
-                                         cudaDataType_t    AType,
-                                         const int         lda,
-                                         const int64_t     strideA,
-                                         const void*       B,
-                                         cudaDataType_t    BType,
-                                         const int         ldb,
-                                         const int64_t     strideB,
-                                         const float       f_beta,
-                                         void*             C,
-                                         cudaDataType_t    CType,
-                                         const int         ldc,
-                                         const int64_t     strideC,
-                                         const int         batch_count,
-                                         cudaDataType_t    computeType)
+                                         const int m,
+                                         const int n,
+                                         const int k,
+                                         const float f_alpha,
+                                         const void* A,
+                                         cudaDataType_t AType,
+                                         const int lda,
+                                         const int64_t strideA,
+                                         const void* B,
+                                         cudaDataType_t BType,
+                                         const int ldb,
+                                         const int64_t strideB,
+                                         const float f_beta,
+                                         void* C,
+                                         cudaDataType_t CType,
+                                         const int ldc,
+                                         const int64_t strideC,
+                                         const int batch_count,
+                                         cudaDataType_t computeType)
 {
     half h_alpha = (half)f_alpha;
-    half h_beta  = (half)f_beta;
+    half h_beta = (half)f_beta;
 
     mu_->lock();
-    int         is_fp16_computeType = computeType == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType == CUDA_R_16F ? 1 : 0;
     const void* alpha =
         is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<const void*>(&f_alpha);
     const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<const void*>(&f_beta);
@@ -545,29 +601,29 @@ void cublasMMWrapper::stridedBatchedGemm(cublasOperation_t transa,
     mu_->unlock();
 }
 
-void cublasMMWrapper::batchedGemm(cublasOperation_t  transa,
-                                  cublasOperation_t  transb,
-                                  const int          m,
-                                  const int          n,
-                                  const int          k,
+void cublasMMWrapper::batchedGemm(cublasOperation_t transa,
+                                  cublasOperation_t transb,
+                                  const int m,
+                                  const int n,
+                                  const int k,
                                   const void* const* A,
-                                  const int          lda,
+                                  const int lda,
                                   const void* const* B,
-                                  const int          ldb,
-                                  void* const*       C,
-                                  const int          ldc,
-                                  const int          batch_count)
+                                  const int ldb,
+                                  void* const* C,
+                                  const int ldc,
+                                  const int batch_count)
 {
     float f_alpha = static_cast<float>(1.0f);
-    float f_beta  = static_cast<float>(0.0f);
+    float f_beta = static_cast<float>(0.0f);
 
     half h_alpha = (half)1.0f;
-    half h_beta  = (half)0.0f;
+    half h_beta = (half)0.0f;
 
     mu_->lock();
-    int         is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
+    int is_fp16_computeType = computeType_ == CUDA_R_16F ? 1 : 0;
     const void* alpha = is_fp16_computeType ? reinterpret_cast<void*>(&h_alpha) : reinterpret_cast<void*>(&f_alpha);
-    const void* beta  = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<void*>(&f_beta);
+    const void* beta = is_fp16_computeType ? reinterpret_cast<void*>(&h_beta) : reinterpret_cast<void*>(&f_beta);
     cublasLtMatmulAlgo_info info = cublas_algo_map_->getAlgo(batch_count, m, n, k, getCublasDataType(Atype_));
 
     check_cuda_error(cublasGemmBatchedEx(cublas_handle_,
@@ -610,12 +666,12 @@ bool cublasMMWrapper::isFuseBatchGemm(const int batch_count, const int m, const
 #ifdef SPARSITY_ENABLED
 void cublasMMWrapper::SpGemm(cublasOperation_t transa,
                              cublasOperation_t transb,
-                             const int         m,
-                             const int         n,
-                             const int         k,
-                             const void*       A,
-                             const void*       B,
-                             void*             C)
+                             const int m,
+                             const int n,
+                             const int k,
+                             const void* A,
+                             const void* B,
+                             void* C)
 {
     if (Atype_ != CUDA_R_16F || Btype_ != CUDA_R_16F || Ctype_ != CUDA_R_16F) {
         throw std::runtime_error("\n[FT][ERROR] sparse GEMM only supports FP16 data type now.");
@@ -627,29 +683,29 @@ void cublasMMWrapper::SpGemm(cublasOperation_t transa,
                "supports FP16 accumulation only.\n");
         not_printed_fp32_accumulation_warning = false;
     }
-    cusparseOrder_t     order = CUSPARSE_ORDER_COL;
+    cusparseOrder_t order = CUSPARSE_ORDER_COL;
     cusparseOperation_t opA = (transa == CUBLAS_OP_N) ? CUSPARSE_OPERATION_NON_TRANSPOSE : CUSPARSE_OPERATION_TRANSPOSE;
     cusparseOperation_t opB = (transb == CUBLAS_OP_N) ? CUSPARSE_OPERATION_NON_TRANSPOSE : CUSPARSE_OPERATION_TRANSPOSE;
     cusparseComputeType compute_type = CUSPARSE_COMPUTE_16F;
-    cusparseLtMatmulDescriptor_t   matmul;
+    cusparseLtMatmulDescriptor_t matmul;
     cusparseLtMatmulAlgSelection_t alg_sel;
-    cusparseLtMatmulPlan_t         plan;
-
-    bool     is_rowmajor    = (order == CUSPARSE_ORDER_ROW);
-    bool     isA_transposed = (opA != CUSPARSE_OPERATION_NON_TRANSPOSE);
-    bool     isB_transposed = (opB != CUSPARSE_OPERATION_NON_TRANSPOSE);
-    auto     num_A_rows     = (isA_transposed) ? k : m;
-    auto     num_A_cols     = (isA_transposed) ? m : k;
-    auto     num_B_rows     = (isB_transposed) ? n : k;
-    auto     num_B_cols     = (isB_transposed) ? k : n;
-    auto     num_C_rows     = m;
-    auto     num_C_cols     = n;
-    unsigned alignment      = 16;
-    auto     lda            = (is_rowmajor) ? num_A_cols : num_A_rows;
-    auto     ldb            = (is_rowmajor) ? num_B_cols : num_B_rows;
-    auto     ldc            = (is_rowmajor) ? num_C_cols : num_C_rows;
-    float    _alpha(1.0f);
-    float    _beta(0.0f);
+    cusparseLtMatmulPlan_t plan;
+
+    bool is_rowmajor = (order == CUSPARSE_ORDER_ROW);
+    bool isA_transposed = (opA != CUSPARSE_OPERATION_NON_TRANSPOSE);
+    bool isB_transposed = (opB != CUSPARSE_OPERATION_NON_TRANSPOSE);
+    auto num_A_rows = (isA_transposed) ? k : m;
+    auto num_A_cols = (isA_transposed) ? m : k;
+    auto num_B_rows = (isB_transposed) ? n : k;
+    auto num_B_cols = (isB_transposed) ? k : n;
+    auto num_C_rows = m;
+    auto num_C_cols = n;
+    unsigned alignment = 16;
+    auto lda = (is_rowmajor) ? num_A_cols : num_A_rows;
+    auto ldb = (is_rowmajor) ? num_B_cols : num_B_rows;
+    auto ldc = (is_rowmajor) ? num_C_cols : num_C_rows;
+    float _alpha(1.0f);
+    float _beta(0.0f);
 
     char mark[256];
     sprintf(mark, "%d_%d_%d_%d", 1, m, n, k);
@@ -703,9 +759,9 @@ void cublasMMWrapper::SpGemm(cublasOperation_t transa,
     CHECK_CUSPARSE(cusparseLtMatmulGetWorkspace(&cusparselt_handle_, &alg_sel, &workspace_size))
     CHECK_CUSPARSE(cusparseLtMatmulPlanInit(&cusparselt_handle_, &plan, &matmul, &alg_sel, workspace_size))
 
-    void*        d_workspace = nullptr;
-    int          num_streams = 1;
-    cudaStream_t streams[1]  = {stream_};
+    void* d_workspace = nullptr;
+    int num_streams = 1;
+    cudaStream_t streams[1] = {stream_};
     CHECK_CUSPARSE(
         cusparseLtMatmul(&cusparselt_handle_, &plan, &_alpha, A, B, &_beta, C, C, d_workspace, streams, num_streams))
     CHECK_CUSPARSE(cusparseLtMatmulPlanDestroy(&plan))
@@ -716,12 +772,12 @@ void cublasMMWrapper::SpGemm(cublasOperation_t transa,
 size_t cublasMMWrapper::getSparseMatrixSize(int m, int k)
 {
     // Get a compressed matrix size of shape (m, k) used in cusparselt.
-    auto            Atype_     = CUDA_R_16F;
-    cusparseOrder_t order      = CUSPARSE_ORDER_COL;
-    unsigned        alignment  = 16;
-    int             num_A_rows = m;
-    int             num_A_cols = k;
-    int             lda        = num_A_rows;
+    auto Atype_ = CUDA_R_16F;
+    cusparseOrder_t order = CUSPARSE_ORDER_COL;
+    unsigned alignment = 16;
+    int num_A_rows = m;
+    int num_A_cols = k;
+    int lda = num_A_rows;
 
     cusparseLtMatDescriptor_t matA;
     CHECK_CUSPARSE(cusparseLtStructuredDescriptorInit(&cusparselt_handle_,
@@ -740,10 +796,10 @@ size_t cublasMMWrapper::getSparseMatrixSize(int m, int k)
 
 void cublasMMWrapper::compressMatrix(const void* input, void* output, const int m, const int k)
 {
-    cusparseOrder_t           order = CUSPARSE_ORDER_COL;
-    cusparseOperation_t       opA   = CUSPARSE_OPERATION_NON_TRANSPOSE;
+    cusparseOrder_t order = CUSPARSE_ORDER_COL;
+    cusparseOperation_t opA = CUSPARSE_OPERATION_NON_TRANSPOSE;
     cusparseLtMatDescriptor_t matA;
-    unsigned                  alignment = 16;
+    unsigned alignment = 16;
     CHECK_CUSPARSE(cusparseLtStructuredDescriptorInit(
         &cusparselt_handle_, &matA, m, k, m, alignment, CUDA_R_16F, order, CUSPARSELT_SPARSITY_50_PERCENT))
     CHECK_CUSPARSE(cusparseLtSpMMACompress2(&cusparselt_handle_, &matA, true, opA, input, output, stream_))
diff --git a/src/fastertransformer/utils/cublasMMWrapper.h b/src/fastertransformer/utils/cublasMMWrapper.h
index 537102b..89496fc 100644
--- a/src/fastertransformer/utils/cublasMMWrapper.h
+++ b/src/fastertransformer/utils/cublasMMWrapper.h
@@ -24,100 +24,118 @@
 #include <mutex>
 #include <string>
 
+#ifdef SPARSITY_HAOJUN
+#include "SpMM_API.cuh"
+#endif
+
 #pragma once
 namespace fastertransformer {
 
 class cublasMMWrapper {
 private:
-    cublasHandle_t   cublas_handle_;
+    cublasHandle_t cublas_handle_;
     cublasLtHandle_t cublaslt_handle_;
 #ifdef SPARSITY_ENABLED
-    cusparseLtHandle_t                               cusparselt_handle_;
+    cusparseLtHandle_t cusparselt_handle_;
     std::map<std::string, cusparseLtMatDescriptor_t> sp_mat_A_desc_map_;
     std::map<std::string, cusparseLtMatDescriptor_t> sp_mat_B_desc_map_;
     std::map<std::string, cusparseLtMatDescriptor_t> sp_mat_C_desc_map_;
 #endif
-
+#ifdef SPARSITY_HAOJUN
+    half* SpMM_ReductionSpace = nullptr;
+    int Bytes_SpMM_ReductionSpace = 0;
+#endif
     cudaDataType_t Atype_;
     cudaDataType_t Btype_;
     cudaDataType_t Ctype_;
     cudaDataType_t computeType_;
 
-    cudaStream_t   stream_;
+    cudaStream_t stream_;
     cublasAlgoMap* cublas_algo_map_;
-    std::mutex*    mu_;
+    std::mutex* mu_;
 
-    IAllocator* allocator_        = nullptr;
-    void*       cublas_workspace_ = nullptr;
+    IAllocator* allocator_ = nullptr;
+    void* cublas_workspace_ = nullptr;
 
     friend class cublasINT8MMWrapper;
 
 public:
-    cublasMMWrapper(cublasHandle_t   cublas_handle_,
+    cublasMMWrapper(cublasHandle_t cublas_handle_,
                     cublasLtHandle_t cublaslt_handle_,
-                    cudaStream_t     stream,
-                    cublasAlgoMap*   map,
-                    std::mutex*      mu,
-                    IAllocator*      allocator);
+                    cudaStream_t stream,
+                    cublasAlgoMap* map,
+                    std::mutex* mu,
+                    IAllocator* allocator);
 
 #ifdef SPARSITY_ENABLED
-    cublasMMWrapper(cublasHandle_t     cublas_handle_,
-                    cublasLtHandle_t   cublaslt_handle_,
+    cublasMMWrapper(cublasHandle_t cublas_handle_,
+                    cublasLtHandle_t cublaslt_handle_,
                     cusparseLtHandle_t cusparselt_handle,
-                    cudaStream_t       stream,
-                    cublasAlgoMap*     map,
-                    std::mutex*        mu,
-                    IAllocator*        allocator);
+                    cudaStream_t stream,
+                    cublasAlgoMap* map,
+                    std::mutex* mu,
+                    IAllocator* allocator);
 #endif
 
     ~cublasMMWrapper();
 
     cublasMMWrapper(const cublasMMWrapper& wrapper);
 
+#ifdef SPARSITY_HAOJUN
+    void SpMM(const unsigned int* Compressed_A,
+              const int* TileOffsets,
+              const void* B,
+              void* C,
+              const int M_Global,
+              const int N_Global,
+              const int K_Global,
+              int Split_K);
+#endif
+
     void Gemm(cublasOperation_t transa,
               cublasOperation_t transb,
-              const int         m,
-              const int         n,
-              const int         k,
-              const void*       alpha,
-              const void*       A,
-              cudaDataType_t    Atype,
-              int               lda,
-              const void*       B,
-              cudaDataType_t    Btype,
-              int               ldb,
-              const void*       beta,
-              void*             C,
-              cudaDataType_t    Ctype,
-              int               ldc,
-              cudaDataType_t    computeType,
-              cublasGemmAlgo_t  algo);
+              const int m,
+              const int n,
+              const int k,
+              const void* alpha,
+              const void* A,
+              cudaDataType_t Atype,
+              int lda,
+              const void* B,
+              cudaDataType_t Btype,
+              int ldb,
+              const void* beta,
+              void* C,
+              cudaDataType_t Ctype,
+              int ldc,
+              cudaDataType_t computeType,
+              cublasGemmAlgo_t algo);
 
     void Gemm(cublasOperation_t transa,
               cublasOperation_t transb,
-              const int         m,
-              const int         n,
-              const int         k,
-              const void*       A,
-              const int         lda,
-              const void*       B,
-              const int         ldb,
-              void*             C,
-              const int         ldc);
+              const int m,
+              const int n,
+              const int k,
+              const void* A,
+              const int lda,
+              const void* B,
+              const int ldb,
+              void* C,
+              const int ldc);
 
     void Gemm(cublasOperation_t transa,
               cublasOperation_t transb,
-              const int         m,
-              const int         n,
-              const int         k,
-              const void*       A,
-              const int         lda,
-              const void*       B,
-              const int         ldb,
-              void*             C,
-              const int         ldc,
-              float             f_alpha,
-              float             f_beta);
+              const int m,
+              const int n,
+              const int k,
+              const void* A,
+              const int lda,
+              const void* B,
+              const int ldb,
+              void* C,
+              const int ldc,
+              float f_alpha,
+              float f_beta);
 
     void setFP32GemmConfig();
     void setFP16GemmConfig();
@@ -133,85 +151,85 @@ public:
 #if (CUDART_VERSION >= 11000)
     void Gemm(cublasOperation_t transa,
               cublasOperation_t transb,
-              const int         m,
-              const int         n,
-              const int         k,
-              const void*       A,
-              const int         lda,
-              const void*       B,
-              const int         ldb,
-              const void*       bias,
-              void*             C,
-              const int         ldc);
+              const int m,
+              const int n,
+              const int k,
+              const void* A,
+              const int lda,
+              const void* B,
+              const int ldb,
+              const void* bias,
+              void* C,
+              const int ldc);
 #endif
 
     void stridedBatchedGemm(cublasOperation_t transa,
                             cublasOperation_t transb,
-                            const int         m,
-                            const int         n,
-                            const int         k,
-                            const void*       A,
-                            const int         lda,
-                            const int64_t     strideA,
-                            const void*       B,
-                            const int         ldb,
-                            const int64_t     strideB,
-                            void*             C,
-                            const int         ldc,
-                            const int64_t     strideC,
-                            const int         batchCount,
-                            const float       f_alpha = 1.0f,
-                            const float       f_beta  = 0.0f);
+                            const int m,
+                            const int n,
+                            const int k,
+                            const void* A,
+                            const int lda,
+                            const int64_t strideA,
+                            const void* B,
+                            const int ldb,
+                            const int64_t strideB,
+                            void* C,
+                            const int ldc,
+                            const int64_t strideC,
+                            const int batchCount,
+                            const float f_alpha = 1.0f,
+                            const float f_beta = 0.0f);
 
     void stridedBatchedGemm(cublasOperation_t transa,
                             cublasOperation_t transb,
-                            const int         m,
-                            const int         n,
-                            const int         k,
-                            const float       f_alpha,
-                            const void*       A,
-                            cudaDataType_t    AType,
-                            const int         lda,
-                            const int64_t     strideA,
-                            const void*       B,
-                            cudaDataType_t    BType,
-                            const int         ldb,
-                            const int64_t     strideB,
-                            const float       f_beta,
-                            void*             C,
-                            cudaDataType_t    CType,
-                            const int         ldc,
-                            const int64_t     strideC,
-                            const int         batch_count,
-                            cudaDataType_t    computeType);
-
-    void batchedGemm(cublasOperation_t  transa,
-                     cublasOperation_t  transb,
-                     const int          m,
-                     const int          n,
-                     const int          k,
+                            const int m,
+                            const int n,
+                            const int k,
+                            const float f_alpha,
+                            const void* A,
+                            cudaDataType_t AType,
+                            const int lda,
+                            const int64_t strideA,
+                            const void* B,
+                            cudaDataType_t BType,
+                            const int ldb,
+                            const int64_t strideB,
+                            const float f_beta,
+                            void* C,
+                            cudaDataType_t CType,
+                            const int ldc,
+                            const int64_t strideC,
+                            const int batch_count,
+                            cudaDataType_t computeType);
+
+    void batchedGemm(cublasOperation_t transa,
+                     cublasOperation_t transb,
+                     const int m,
+                     const int n,
+                     const int k,
                      const void* const* A,
-                     const int          lda,
+                     const int lda,
                      const void* const* B,
-                     const int          ldb,
-                     void* const*       C,
-                     const int          ldc,
-                     const int          batch_count);
+                     const int ldb,
+                     void* const* C,
+                     const int ldc,
+                     const int batch_count);
 
     bool isFuseBatchGemm(const int batch_count, const int m, const int k, const int n);
 
 #ifdef SPARSITY_ENABLED
     void SpGemm(cublasOperation_t transa,
                 cublasOperation_t transb,
-                const int         m,
-                const int         n,
-                const int         k,
-                const void*       A,
-                const void*       B,
-                void*             C);
+                const int m,
+                const int n,
+                const int k,
+                const void* A,
+                const void* B,
+                void* C);
 
     size_t getSparseMatrixSize(int m, int k);
-    void   compressMatrix(const void* input, void* output, const int m, const int k);
+    void compressMatrix(const void* input, void* output, const int m, const int k);
 
     bool isUseSparse(const int batch_count, const int m, const int n, const int k);
 #endif
diff --git a/src/fastertransformer/utils/memory_utils.cu b/src/fastertransformer/utils/memory_utils.cu
index ffe46f3..142cbb5 100644
--- a/src/fastertransformer/utils/memory_utils.cu
+++ b/src/fastertransformer/utils/memory_utils.cu
@@ -23,7 +23,9 @@ namespace fastertransformer {
 template<typename T>
 void deviceMalloc(T** ptr, size_t size, bool is_random_initialize)
 {
+#ifndef SPARSITY_HAOJUN
     FT_CHECK_WITH_INFO(size >= 0, "Ask deviceMalloc size " + std::to_string(size) + "< 0 is invalid.");
+#endif
     check_cuda_error(cudaMalloc((void**)(ptr), sizeof(T) * size));
     if (is_random_initialize) {
         cudaRandomUniform(*ptr, size);
@@ -69,6 +71,9 @@ template void deviceFree(__nv_bfloat16*& ptr);
 #endif
 template void deviceFree(unsigned short*& ptr);
 template void deviceFree(int*& ptr);
+#ifdef SPARSITY_HAOJUN
+template void deviceFree(unsigned int*& ptr);
+#endif
 template void deviceFree(bool*& ptr);
 template void deviceFree(char*& ptr);
 template void deviceFree(int8_t*& ptr);
@@ -174,7 +179,7 @@ cudaAutoCpy(unsigned long long const** tgt, unsigned long long const* const* src
 template<typename T>
 __global__ void cuda_random_uniform_kernel(T* buffer, const int size)
 {
-    const int     idx = blockIdx.x * blockDim.x + threadIdx.x;
+    const int idx = blockIdx.x * blockDim.x + threadIdx.x;
     curandState_t local_state;
     curand_init((unsigned long long int)1337, idx, 0, &local_state);
     for (int index = idx; index < size; index += blockDim.x * gridDim.x) {
@@ -256,9 +261,113 @@ __host__ __device__ inline half convert_to_type<__nv_bfloat16, half>(__nv_bfloat
 }
 #endif  // ENABLE_BF16
 
+#ifdef SPARSITY_HAOJUN
+template<typename T>
+int loadDataArrayFromBin(T* TargetArrayPTR, int NumElem, std::string FileName)
+{
+    if (NumElem < 0) {
+        printf("loadDataArrayFromBin: NumElem < 0, Error!\n");
+        exit(-1);
+    }
+    if (NumElem == 0) {
+        printf("loadDataArrayFromBin: NumElem = 0, skipping loading this data.\n");
+        return 0;
+    }
+    std::vector<T> host_array(NumElem);
+    std::ifstream in(FileName, std::ios::in | std::ios::binary);
+    if (!in.is_open()) {
+        FT_LOG_WARNING("file %s cannot be opened, loadDataArrayFromBin fails. \n", FileName.c_str());
+        return 0;
+    }
+    size_t loaded_data_size = sizeof(T) * NumElem;
+    in.seekg(0, in.end);
+    in.seekg(0, in.beg);
+
+    FT_LOG_DEBUG("Read " + std::to_string(loaded_data_size) + " bytes from " + FileName);
+    in.read((char*)host_array.data(), loaded_data_size);
+
+    // if(std::is_same<int, T>::value)
+    //{
+    //    printf("Printing TileOffsets...\n");
+    //    for(int i=0; i<10; i++)
+    //    {
+    //        printf("%d ", host_array[i] );
+    //    }
+    //    printf("\n");
+    //}
+
+    size_t in_get_size = in.gcount();
+    if (in_get_size != loaded_data_size) {
+        FT_LOG_WARNING("file %s only has %ld, but request %ld, loading model fails! \n",
+                       FileName.c_str(),
+                       in_get_size,
+                       loaded_data_size);
+        return 0;
+    }
+    // cudaDeviceSynchronize();
+    check_cuda_error(cudaMemcpy(TargetArrayPTR, (T*)host_array.data(), sizeof(T) * NumElem, cudaMemcpyHostToDevice));
+
+    in.close();
+    return 0;
+}
+
+template int loadDataArrayFromBin<int>(int* TargetArrayPTR, int NumElem, std::string FileName);
+template int loadDataArrayFromBin<unsigned int>(unsigned int* TargetArrayPTR, int NumElem, std::string FileName);
+
+/*
+TargetArrayPTR is CPU ptr instead of GPU ptr, which is different from loadWeightFromBinFunc().
+*/
+template<typename T>
+int loadDataArrayFromBin_To_CPU(T* TargetArrayPTR, int NumElem, std::string FileName)
+{
+    if (NumElem < 0) {
+        printf("loadDataArrayFromBin: NumElem < 0, Error!\n");
+        exit(-1);
+    }
+    if (NumElem == 0) {
+        printf("loadDataArrayFromBin: NumElem = 0, skipping loading this data.\n");
+        printf("FileName: %s\n", FileName.c_str());
+        return 0;
+    }
+    std::ifstream in(FileName, std::ios::in | std::ios::binary);
+    if (!in.is_open()) {
+        FT_LOG_WARNING("file %s cannot be opened, loadDataArrayFromBin fails. \n", FileName.c_str());
+        return 0;
+    }
+    size_t loaded_data_size = sizeof(T) * NumElem;
+    in.seekg(0, in.end);
+    in.seekg(0, in.beg);
+
+    FT_LOG_DEBUG("Read " + std::to_string(loaded_data_size) + " bytes from " + FileName);
+    in.read((char*)TargetArrayPTR, loaded_data_size);
+
+    size_t in_get_size = in.gcount();
+    if (in_get_size != loaded_data_size) {
+        FT_LOG_WARNING("file %s only has %ld, but request %ld, loading model fails! \n",
+                       FileName.c_str(),
+                       in_get_size,
+                       loaded_data_size);
+        return 0;
+    }
+
+    in.close();
+    return 0;
+}
+
+template int loadDataArrayFromBin_To_CPU<int>(int* TargetArrayPTR, int NumElem, std::string FileName);
+template int loadDataArrayFromBin_To_CPU<unsigned int>(unsigned int* TargetArrayPTR, int NumElem, std::string FileName);
+
+#endif
+
 template<typename T, typename T_IN>
 int loadWeightFromBinFunc(T* ptr, std::vector<size_t> shape, std::string filename)
 {
+#ifdef SPARSITY_HAOJUN
+    if (ptr == nullptr) {
+        printf("[Haojun Warning] Skip loadWeightFromBinFunc: ptr is nullptr!\n");
+        return 0;
+    }
+#endif
     if (shape.size() > 2) {
         printf("[ERROR] shape should have less than two dims \n");
         return -1;
@@ -273,9 +382,11 @@ int loadWeightFromBinFunc(T* ptr, std::vector<size_t> shape, std::string filenam
         return 0;
     }
     std::vector<T_IN> host_array(size);
-    std::ifstream     in(filename, std::ios::in | std::ios::binary);
+    std::ifstream in(filename, std::ios::in | std::ios::binary);
     if (!in.is_open()) {
+#ifdef PRINT_MISSING_INPUT_FILES
         FT_LOG_WARNING("file %s cannot be opened, loading model fails! \n", filename.c_str());
+#endif
         return 0;
     }
 
@@ -322,9 +433,9 @@ template int
 loadWeightFromBinFunc<__nv_bfloat16, half>(__nv_bfloat16* ptr, std::vector<size_t> shape, std::string filename);
 template int loadWeightFromBinFunc<float, __nv_bfloat16>(float* ptr, std::vector<size_t> shape, std::string filename);
 template int loadWeightFromBinFunc<half, __nv_bfloat16>(half* ptr, std::vector<size_t> shape, std::string filename);
-template int loadWeightFromBinFunc<__nv_bfloat16, __nv_bfloat16>(__nv_bfloat16*      ptr,
+template int loadWeightFromBinFunc<__nv_bfloat16, __nv_bfloat16>(__nv_bfloat16* ptr,
                                                                  std::vector<size_t> shape,
-                                                                 std::string         filename);
+                                                                 std::string filename);
 #endif  // ENABLE_BF16
 
 template<typename T>
@@ -429,8 +540,8 @@ __global__ void fakeCast(T_IN* input_ptr, const size_t size)
 {
     for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < size; i += blockDim.x * gridDim.x) {
         T_fake_type tmp_val = (T_fake_type)((float)input_ptr[i]);
-        tmp_val             = tmp_val * (T_fake_type)(1.0f);
-        input_ptr[i]        = (T_IN)((float)tmp_val);
+        tmp_val = tmp_val * (T_fake_type)(1.0f);
+        input_ptr[i] = (T_IN)((float)tmp_val);
     }
 }
 
diff --git a/src/fastertransformer/utils/memory_utils.h b/src/fastertransformer/utils/memory_utils.h
index abd9258..059a53c 100644
--- a/src/fastertransformer/utils/memory_utils.h
+++ b/src/fastertransformer/utils/memory_utils.h
@@ -47,11 +47,18 @@ void cudaAutoCpy(T* tgt, const T* src, const int size, cudaStream_t stream = NUL
 template<typename T>
 void cudaRandomUniform(T* buffer, const int size);
 
+#ifdef SPARSITY_HAOJUN
 template<typename T>
-int loadWeightFromBin(T*                  ptr,
+int loadDataArrayFromBin(T* TargetArrayPTR, int NumElem, std::string FileName);
+template<typename T>
+int loadDataArrayFromBin_To_CPU(T* TargetArrayPTR, int NumElem, std::string FileName);
+#endif
+
+template<typename T>
+int loadWeightFromBin(T* ptr,
                       std::vector<size_t> shape,
-                      std::string         filename,
-                      FtCudaDataType      model_file_type = FtCudaDataType::FP32);
+                      std::string filename,
+                      FtCudaDataType model_file_type = FtCudaDataType::FP32);
 
 void invokeCudaD2DcpyHalf2Float(float* dst, half* src, const int size, cudaStream_t stream);
 void invokeCudaD2DcpyFloat2Half(half* dst, float* src, const int size, cudaStream_t stream);
-- 
2.25.1

